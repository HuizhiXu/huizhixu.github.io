<!DOCTYPE html>
<html lang="zh-cn"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">2023-02-16 如何理解Seq2seq | 徐慧志的个人博客</title>
<meta property="og:title" content="2023-02-16 如何理解Seq2seq | 徐慧志的个人博客" />
<meta name="twitter:title" content="2023-02-16 如何理解Seq2seq | 徐慧志的个人博客" />
<meta itemprop="name" content="2023-02-16 如何理解Seq2seq | 徐慧志的个人博客" />
<meta name="application-name" content="2023-02-16 如何理解Seq2seq | 徐慧志的个人博客" />
<meta property="og:site_name" content="" />

<meta name="description" content="有encoder和decoder就可以说这是一个Seq2seq模型">
<meta itemprop="description" content="有encoder和decoder就可以说这是一个Seq2seq模型" />
<meta property="og:description" content="有encoder和decoder就可以说这是一个Seq2seq模型" />
<meta name="twitter:description" content="有encoder和decoder就可以说这是一个Seq2seq模型" />

<meta property="og:locale" content="zh-cn" />
<meta name="language" content="zh-cn" />

  <link rel="alternate" hreflang="zh-cn" href="http://localhost:1313/zh-cn/know_how/20230216%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3seq2seq/" title="中文 (简体)" />






<meta name="generator" content="Hugo 0.135.0">

    
    <meta property="og:url" content="http://localhost:1313/zh-cn/know_how/20230216%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3seq2seq/">
  <meta property="og:site_name" content="徐慧志的个人博客">
  <meta property="og:title" content="2023-02-16 如何理解Seq2seq">
  <meta property="og:description" content="有encoder和decoder就可以说这是一个Seq2seq模型">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="know_how">
    <meta property="article:published_time" content="2023-02-16T18:31:50+08:00">
    <meta property="article:modified_time" content="2023-03-30T00:00:00+00:00">
    <meta property="article:tag" content="Tech">
    <meta property="article:tag" content="Ai">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="2023-02-16 如何理解Seq2seq">
  <meta name="twitter:description" content="有encoder和decoder就可以说这是一个Seq2seq模型">


    

    <link rel="canonical" href="http://localhost:1313/zh-cn/know_how/20230216%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3seq2seq/">
    <link href="../../../style.min.e390ba7da26222f4dc42a349955d76dbbe44e5ce2535a43de5a70633a0a9ec3c.css" rel="stylesheet">
    <link href="../../../code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="../../../icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../icons/favicon-16x16.png">
    <link rel="mask-icon" href="../../../icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="../../../favicon.ico">




<link rel="manifest" href="http://localhost:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">
    <meta name="color-scheme" content="light dark">

    
    <link rel="icon" type="image/svg+xml" href="../../../icons/favicon.svg">

    
    
</head>
<body data-theme = "" class="notransition">

<script src="../../../js/theme.js"></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="http://localhost:1313/zh-cn/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>首页</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="../../../know_how/">
                        技术
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../life/">
                        生活见闻
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../page/about/">
                        关于
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../link/">
                        宝藏集结
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../tags/">
                        分类
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">2023-02-16 如何理解Seq2seq</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2023-02-16T18:31:50&#43;08:00" itemprop="datePublished"> Feb 16, 2023 </time>
                </div>
                
            </header>
            
    
    <details class="toc" ZgotmplZ>
        <summary><b>目录</b></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#结构和原理">结构和原理</a>
      <ul>
        <li><a href="#encoder">Encoder</a></li>
        <li><a href="#decoder">Decoder</a></li>
        <li><a href="#decoder和encoder对比">Decoder和Encoder对比</a></li>
        <li><a href="#encoder-decoder">Encoder-Decoder</a></li>
      </ul>
    </li>
    <li><a href="#如何做训练">如何做训练</a></li>
    <li><a href="#应用">应用</a></li>
    <li><a href="#参考">参考：</a></li>
  </ul>
</nav>
    </details>
            <div class="page-content">
                <p>先搞清楚几个基本概念：</p>
<p>Seq2seq是一个概念，它的表现形式就是有encoder和decoder的一个结构。换言之，有encoder和decoder就可以说这是一个Seq2seq模型。编码器或者解码器具体可以用CNN、RNN、LSTM或者attention来构建。</p>
<p>transformer是一种基于Attention的Seq2seq。</p>
<h1 id="seq2seq">Seq2seq</h1>
<ul>
<li>input是一个sequence， output也是一个sequence，但是维度由模型决定。</li>
<li>例子：
<ul>
<li>语音辨识：一串语音转为”今晚吃什么？“几个文字。</li>
<li>机器翻译</li>
<li>语音翻译：输入machine learning，输出”机器学习”。
<ul>
<li>为何不将”语音辨识“和”机器翻译“结合起来，因为有的语言没有文字</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="结构和原理">结构和原理</h2>
<p>Seq2seq由一个encoder和一个decoder决定</p>
<h3 id="encoder">Encoder</h3>
<ol>
<li>input是一个sequence， output也是一个sequence。</li>
<li>input vector加上positional encoding，然后经过multi-head attention，然后进行residual + layer normalization，然后经过FC，再做一次Add&amp;Norm，是这个encoder的输出。这个过程会重复。</li>
</ol>
<p><img alt="0" src="../../../img/20230216/0.png"></p>
<ol>
<li>更细节的设计：</li>
</ol>
<ul>
<li>input vector进来之后，经过self-attention，input和output相加，然后进行一层layer normalization。然后进入FC层，再进行一次Add&amp;Norm（和自己相加&amp;Normalization），这个输出就是一个block的输出。</li>
</ul>
<p><img alt="1" src="../../../img/20230216/1.png"></p>
<ul>
<li>residual connection：输入与输出相加，防止层级过高导致的梯度消失。</li>
<li>layer normalization：
<ul>
<li>输入一个向量，输出一个向量，不需要考虑batch中其他的向量。</li>
<li>对同一个feature，同一个example不同的dimension去计算。（这里feature就是example）</li>
<li>做法：计算它 的mean $m$和standard deviation $\sigma$</li>
</ul>
</li>
</ul>
<p>$$
x_i^\prime =\frac {x_i - m} {\sigma}
$$</p>
<ul>
<li>batch normalization：
<ul>
<li>对同一个dimension，不同的example，不同的feature去计算mean和standard deviation</li>
</ul>
</li>
</ul>
<h3 id="decoder">Decoder</h3>
<ol>
<li>decoder有两种，一种叫auto-regressive。这里讲的都是auto-regressive。</li>
<li>decoder的输入：
<ul>
<li>在前边先加一个特殊的符号：BOS。</li>
<li>每个输入可以表示成一个one hot vector。例如”机器学习“加上BOS就是5个one hot vector。</li>
</ul>
</li>
<li>decoder的输出：
<ul>
<li>
<p>想好decoder输出的单位是什么，假设我们做的是中文的语音辨识，那么decoder输出的就是中文，那么vocabulary就是中文的数目，常用4000个字。不同的语言输出的单位不一样，英文可以输出字母，word， subword作为单位。</p>
</li>
<li>
<p>1个Input vector进去之后，出来1个output vector，它的长度和vocabulary的size是一样的。他会给vocabulary的每一个单位一个分数，分数最高的就是最后的输出。5个input vector，出来n个output vector。(n需要decoder自己决定)</p>
<p><img alt="2" src="../../../img/20230216/2.png"></p>
</li>
</ul>
</li>
</ol>
<ul>
<li>
<p>decoder看到的输入，其实就是前一个时间自己的输出。</p>
<ul>
<li>这里有一个问题，如果输出错误，那么输入也会错误，会造成error propagation。</li>
</ul>
<p><img alt="3" src="../../../img/20230216/3.png"></p>
</li>
</ul>
<h3 id="decoder和encoder对比">Decoder和Encoder对比</h3>
<ol>
<li>decoder与encoder结构类似</li>
<li>decoder有一层masked multi-head attention</li>
</ol>
<p><img alt="4" src="../../../img/20230216/4.png"></p>
<p>之前的self-attention，都要看过完整的input之后才做决定。$b^1$是由$a^1$到$a^4$一起决定的。</p>
<p>masked attention 的意思是：</p>
<p>产生$b^1$的时候，只能用$a^1$。</p>
<p>产生$b^2$的时候，只能用$a^1$和$a^2$。</p>
<p><img alt="5" src="../../../img/20230216/5.png"></p>
<ol>
<li>为什么要masked？</li>
</ol>
<p>想想decoder是如何运作的的。decoder的输出是一个一个产生的。先有$a^1$，然后有$a^2$。再计算$b^1$的时候，是没有$a^2$的，是没有办法把$a^2$计算进去的。</p>
<p>encoder是一次性把$a^1$到$a^4$一起读进去。</p>
<p><img alt="6" src="../../../img/20230216/6.png"></p>
<ol>
<li>decoder必须自己决定输出的长度。那么如何决定呢
<ul>
<li>加一个stop token。decoder看到这个符号就停止。</li>
</ul>
</li>
</ol>
<h3 id="encoder-decoder">Encoder-Decoder</h3>
<ol>
<li>decoder结构中有一个cross attention，它的输入中有两个是来自encoder的输出，一个是来源自己。</li>
</ol>
<p><img alt="7" src="../../../img/20230216/7.png"></p>
<ol>
<li>encoder产生kv，decoder产生q</li>
</ol>
<h2 id="如何做训练">如何做训练</h2>
<ol>
<li>假设做语音识别，要有训练资料，要收集一堆语音资料。</li>
<li>我们期待，把begin丢给decoder的时候，它的第一个输出，应该要跟“机”越接近越好。在ground truth里面， “机”这个字会被表示成一个one hot vector。decoder的输出是一个distribution，是一个几率的分布，我们希望这个分布能够接近“机”的one hot vector，所以要去计算one hot vector和这个分布的cross entropy，然后希望这个cross entropy越小越好。在产生完所有的输出之后，我们希望这个总的cross entropy越小越好。</li>
</ol>
<p>这个计算很接近分类。可以理解为，每一次decoder在产生一个token输出的时候，就是做了一次分类。</p>
<ol>
<li>decoder的输入是正确答案。</li>
</ol>
<p>也就是说，在有Begin的情况下，输出“机”，在有begin和”机”的情况下，输出 “器”，在有begin、“机”和”器”的情况下，输出“学”。</p>
<p>teacher forcing：在训练的时候，using the ground truth as input</p>
<ol>
<li>训练的时候decoder的输入给的是正确答案，推断的时候有可能出错，这里有一个mismatch。
<ol>
<li>如何解决：在训练的时候也给一些错误的输入，可能会学的更好，这个叫scheduled sampling。</li>
<li>但是scheduled sampling，会伤害到transformer的平行能力。transformer有自己的scheduled sampling</li>
</ol>
</li>
</ol>
<p><img alt="8" src="../../../img/20230216/8.png"></p>
<ol>
<li>训练tips
<ul>
<li>Copy Mechanism：decoder没有必要创造输出，可以直接从输入的东西里面复制。
<ul>
<li>在做Chat-Bot：-你好，我是小花。-小花你好，欢迎你！</li>
<li>在做摘要：
<ul>
<li>数据量：要训练这种，需要百万篇文章（把标题当做摘要）</li>
<li>从课文里面复制一些句子出来，这种叫pointer network。</li>
</ul>
</li>
</ul>
</li>
<li>Guided Attention
<ul>
<li>看到中文的句子，输出这段语音。有时候语音会漏字。
<ul>
<li>input and output monotonically aligned。强制它monotonic attention</li>
</ul>
</li>
<li>对于有位置需求的语音：先说第一个字，再说第二个字等等。
<ul>
<li>location-aware attention</li>
</ul>
</li>
</ul>
</li>
<li>Beam Search
<ul>
<li>假设decoder只能输出A和B两个字。那么decoder会每一次选择输出A还是B。 根据概率最大的选择，叫做greedy search。</li>
<li>greedy search不是最优，全局搜索也不可能。</li>
<li>解决方法就是beam search。（这里没懂）</li>
</ul>
</li>
</ul>
</li>
<li>优化 Evaluation metrics
<ol>
<li>评估的标准，Bleu score是decoder先产生一个完整的句子之后，再去跟ground truth的句子比较。</li>
<li>训练的时候，每一个token的输出都是单独产生的。那么minimize cross entropy真的能够使bleu score最好吗？不一定。
<ol>
<li>所以评判标准不是cross entropy 而是bleu score</li>
<li>可不可以在training的时候就最大化bleu score呢？ 不能，因为它不能微分</li>
<li>这时，可以用强化学习 RL</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="应用">应用</h2>
<ol>
<li>question answering ：
<ul>
<li>很多问题NLP的任务都可以转化为问答QA，而QA可以用Seq2seq模型来解决。</li>
<li>原理：输入是question和context拼接起来，输出是answer。</li>
<li>例子：
<ul>
<li>机器翻译-这个句子的德文翻译是什么</li>
<li>摘要：这段文字的摘要是什么？</li>
<li>情感分析：这篇文章是正面还是负面？</li>
</ul>
</li>
<li>对多数nlp任务，专门的模型比seq2seq 会得到更好的结果。</li>
</ul>
</li>
</ol>
<p><img alt="9" src="../../../img/20230216/9.png"></p>
<ol>
<li>
<p>句法分析：syntactic parsing 也可以用seq2seq来做。树可以用括号链接文字来表示。</p>
<p><img alt="10" src="../../../img/20230216/10.png"></p>
</li>
<li>
<p>multi-label classification： 也可以用seq2seq</p>
<ul>
<li>multi-class classification： 有不止一个class，机器得选（有且只能）一个class出来</li>
<li>multi-label classification：一个事物可以有=属于不同的class</li>
</ul>
<p><img alt="11" src="../../../img/20230216/11.png"></p>
</li>
<li>
<p>object detection ：也可以用seq2seq</p>
</li>
</ol>
<h2 id="参考">参考：</h2>
<ul>
<li>
<p><a href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/">李宏毅老师的课程链接</a></p>
</li>
<li>
<p><a href="https://www.bilibili.com/video/BV1v3411r78R?p=4&vd_source=bbd38c44460fafa204a3540d4f8a2657">【李宏毅机器学习2021】自注意力机制 (Transformer) (下)_哔哩哔哩_bilibili</a></p>
</li>
</ul>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
</div>
    <small class="footer_copyright">
        © 2025 .
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noopener">Hugo blog awesome</a>.
    </small>
</footer>







    
    <script async src="http://localhost:1313/js/main.js" ></script>

    

</body>
</html>
