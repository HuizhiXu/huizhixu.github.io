<!DOCTYPE html>
<html lang="zh-cn"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">2023-02-09 如何理解自注意力机制 | 徐慧志的个人博客</title>
<meta property="og:title" content="2023-02-09 如何理解自注意力机制 | 徐慧志的个人博客" />
<meta name="twitter:title" content="2023-02-09 如何理解自注意力机制 | 徐慧志的个人博客" />
<meta itemprop="name" content="2023-02-09 如何理解自注意力机制 | 徐慧志的个人博客" />
<meta name="application-name" content="2023-02-09 如何理解自注意力机制 | 徐慧志的个人博客" />
<meta property="og:site_name" content="" />

<meta name="description" content="Attention is all you need">
<meta itemprop="description" content="Attention is all you need" />
<meta property="og:description" content="Attention is all you need" />
<meta name="twitter:description" content="Attention is all you need" />

<meta property="og:locale" content="zh-cn" />
<meta name="language" content="zh-cn" />

  <link rel="alternate" hreflang="zh-cn" href="http://localhost:1313/zh-cn/know_how/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="中文 (简体)" />






<meta name="generator" content="Hugo 0.135.0">

    
    <meta property="og:url" content="http://localhost:1313/zh-cn/know_how/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">
  <meta property="og:site_name" content="徐慧志的个人博客">
  <meta property="og:title" content="2023-02-09 如何理解自注意力机制">
  <meta property="og:description" content="Attention is all you need">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="know_how">
    <meta property="article:published_time" content="2023-02-09T08:31:50+08:00">
    <meta property="article:modified_time" content="2023-03-30T00:00:00+00:00">
    <meta property="article:tag" content="Tech">
    <meta property="article:tag" content="Ai">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="2023-02-09 如何理解自注意力机制">
  <meta name="twitter:description" content="Attention is all you need">


    

    <link rel="canonical" href="http://localhost:1313/zh-cn/know_how/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">
    <link href="../../../style.min.e390ba7da26222f4dc42a349955d76dbbe44e5ce2535a43de5a70633a0a9ec3c.css" rel="stylesheet">
    <link href="../../../code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="../../../icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../icons/favicon-16x16.png">
    <link rel="mask-icon" href="../../../icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="../../../favicon.ico">




<link rel="manifest" href="http://localhost:1313/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">
    <meta name="color-scheme" content="light dark">

    
    <link rel="icon" type="image/svg+xml" href="../../../icons/favicon.svg">

    
    
</head>
<body data-theme = "" class="notransition">

<script src="../../../js/theme.js"></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="http://localhost:1313/zh-cn/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title>首页</title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="../../../know_how/">
                        技术
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../life/">
                        生活见闻
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../page/about/">
                        关于
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../link/">
                        宝藏集结
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="../../../tags/">
                        分类
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">2023-02-09 如何理解自注意力机制</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2023-02-09T08:31:50&#43;08:00" itemprop="datePublished"> Feb 9, 2023 </time>
                </div>
                
            </header>
            
    
    <details class="toc" ZgotmplZ>
        <summary><b>目录</b></summary>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#理解输入与输出">理解输入与输出</a></li>
    <li><a href="#一个vector对应一个label的情况即输入和输出一样多也叫做sequence-labeling">一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling</a></li>
    <li><a href="#self-attention">Self-attention</a>
      <ul>
        <li><a href="#运作原理">运作原理：</a></li>
      </ul>
    </li>
    <li><a href="#qkv从矩阵的角度看self-attention">$QKV$（从矩阵的角度看self-attention）</a></li>
    <li><a href="#怎么得到wqwk和wv">怎么得到$W^q$、$W^k$和$W^v$</a></li>
    <li><a href="#自注意模块计算两个向量的关联性">自注意模块计算两个向量的关联性</a>
      <ul>
        <li><a href="#内积方法"><strong>内积方法</strong></a></li>
        <li><a href="#相加方法"><strong>相加方法</strong></a></li>
      </ul>
    </li>
    <li><a href="#参考">参考：</a></li>
  </ul>
</nav>
    </details>
            <div class="page-content">
                <h2 id="理解输入与输出">理解输入与输出</h2>
<ul>
<li>输入有可能是一个 vector，有可能是多个 vector</li>
<li>输出：
<ul>
<li>一个序列对应一个 label。the whole sequence has a label
<ul>
<li>例子：在情感分析里面，This is good 对应的输入是多个 vector，输出为 positive，是一个vector。</li>
</ul>
</li>
<li>一个 vector 对应一个 label。一个序列对应多个 label。
<ul>
<li>例子：在词性标注里面，This is good 对应的输入是多个 vector，输出为 代词，动词，形容词。</li>
</ul>
</li>
<li>模型决定 label 的个数。seq2seq 任务
<ul>
<li>例子：在机器翻译里面，This is good 对应的输入是3个 vector，中文翻译是”不错“，输出为2个 vector。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="一个vector对应一个label的情况即输入和输出一样多也叫做sequence-labeling">一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling</h2>
<ul>
<li>例子： I saw a saw</li>
<li>如何解决 sequence labeling 的问题：用 fully connected network 对每一个 input vector 进行作用</li>
<li>弊端：
<ul>
<li>用 fully connected network 来输出，假设对 I saw a saw 做词性标注。对于 FC 层来说，两个 saw没有什么不同，但是他们实际上一个是动词，一个是名词。</li>
</ul>
</li>
<li>解决思路：考虑更多的上下文。每一个 fc 层，都对所有的输入作用。或者给他一个 window，作用于相邻的几个 input vector。但是作用还是有限，计算也很复杂。</li>
</ul>
<p>我们想考虑整个 sequence，但是不想把 sequence 所有的数据都包括在里面，就有了 self-attention。</p>
<h2 id="self-attention">Self-attention</h2>
<p>如果要考虑上下文的信息，输入可以扔进 self-attention 之后，生成 vector with context，然后丢进FC层，然后再过一次 self-attention，然后再丢进 FC 层。</p>
<p>可以这样理解，self-attention 获得上下文信息，FC 层专注于局部信息，交替使用。</p>
<h3 id="运作原理">运作原理：</h3>
<ul>
<li>输入：是一串 vector，可以是整个 network 的输入，也可以是 FC 层的输出，这里用 $a$ 表示。</li>
<li>输出：一串 vector，这里用  $b$ 表示。每一个 $b$  都对所有 $a$ 作用。（这个图乍一看很像上面的 FC，但其实不一样）</li>
</ul>
<p><img alt="0" src="../../../img/20230209/0.png"></p>
<p>问题就变成了，输入是 $a$ 时，如何计算 $b$？用 $b^1$来说明，具体分三步。</p>
<ul>
<li>如何产生 $b^1$ 这个向量：
<ul>
<li>
<p>第一步：找出 sequence 里面跟 $a^1$ 相关的其他向量。（FC 层会把 sequence 所有的向量都包括进来，这里只包括相关的向量，这就是不同点。）这个机制叫做自注意机制。</p>
<ul>
<li>每一个向量跟 $a^1$ 相关联的程度，我们用数值 $\alpha$ 来表示。</li>
<li>Self-attention 的 module 怎么决定两个向量的关联性呢？
<ul>
<li>内积：dot product （方法原理在文章最后面）</li>
<li>相加：additive（方法原理在文章最后面）</li>
</ul>
</li>
<li>这里的做法是：分别计算 $a^1$ 与 $a^2$， $a^3$，$a^4$ 的关联性</li>
<li>把 $a^1$ 乘以 $W^q$，得到 $q^1$ ，$q^1$ 就是 query——搜寻。</li>
<li>把 $a^2$ 乘以 $W^k$，得到 $k^2$，$k^2$ 就是 key——键。把 $a^3$ 乘以 $W^k$，得到 $k^3$，把 $a^4$ 乘以 $W^k$，得到 $k^4$。</li>
<li>那么 $\alpha_{1,2} =q^1\cdot k^2$，关联性就算出来了。$\alpha_{1,2}$ 也叫 attention score。</li>
<li>同理可以算出 $\alpha_{1,3}，$$\alpha_{1,4}$。</li>
<li>一般我们也会算 $q^1$ 和 $k^1$ 的关联性，也就是 $\alpha_{1,1}$。</li>
</ul>
<p><img alt="1" src="../../../img/20230209/1.png"></p>
</li>
<li>
<p>第二步：对 $\alpha_{1,1}$，$\alpha_{1,2}$，$\alpha_{1,3}$，$\alpha_{1,4}$进行Softmax 操作，得到$\alpha^\prime$。</p>
<ul>
<li>为什么要用 Softmax：Softmax 最常见，这里也可以用其他函数，可以用 relu 等。</li>
</ul>
</li>
<li>
<p>第三步：根据$\alpha^\prime$，在 sequence 里面抽取重要的信息。</p>
<ul>
<li>做法：将 $a^1$，$a^2$，$a^3$，$a^4$ 分别乘以 $W^v$，得到 $v^1，$$v^2，$$v^3$，$v^4$。  （其实这里$v^1，v^2，v^3，v^4$就是$a^1，a^2，a^3，a^4$自己本身的等价表示）</li>
<li>将 $v^1$ 到 $v^4$ 都乘以分别的 $\alpha^\prime$， 然后再相加。</li>
</ul>
<p>$$
b^1  = \sum_i \alpha\prime_{1,i} v^i
$$</p>
<ul>
<li>
<p>如果 $a^1$ 和 $a^2$ 的关联性很强，$\alpha^\prime_{1,2}$ 的值很大，那么经过 weighted sum 得到的 $b^1$ 的值就可能会接近 $v^2$。</p>
<p>也就是说，谁的 attention score 越大，谁的 v 就会 dominate 抽出来的结果。</p>
</li>
</ul>
<p><img alt="2" src="../../../img/20230209/2.png"></p>
</li>
</ul>
</li>
</ul>
<h2 id="qkv从矩阵的角度看self-attention">$QKV$（从矩阵的角度看self-attention）</h2>
<ol>
<li>把 $a^1$ 到 $a^4$ 看成是矩阵 $I$ 的列，$q^1$ 到 $q^4$ 是矩阵 $Q$ 的列，那么可以转化成矩阵间的运算。</li>
</ol>
<p>矩阵$I$乘以$Wq$，得到矩阵$Q$，$Q$的4列就是$q^1$到$q^4$。</p>
<p>同样，输入矩阵I乘以$W^k$，得到矩阵$K$，$K$的4列就是$k^1$到$k^4$。</p>
<p>输入$I$乘上三个不同的矩阵，就得到了$QKV$。</p>
<p><img alt="3" src="../../../img/20230209/3.png"></p>
<ol>
<li>
<p>每一个 $q$ 会和每一个 $k$ 计算内积，得到 attention score。</p>
<ol>
<li>
<p>矩阵和向量相乘：</p>
<ol>
<li>矩阵 $K$ 和 $q^1$ 内积，得到 $a^1$ 和其他输入的相关性。</li>
<li>矩阵 $K$ 和 $q^2$ 内积，得到 $a^2$ 和其他输入的相关性。</li>
<li>合起来，就是</li>
</ol>
<p>$$
\begin{array}{cc}<br>
\alpha_{1,1} &amp; \alpha_{2,1} &amp; \alpha_{3,1} &amp; \alpha_{4,1} \
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{3,2} &amp; \alpha_{4,2} \
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp; \alpha_{4,3}\
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp; \alpha_{4,4}
\end{array} =</p>
<p>\begin{array}{cc}
k^1 \
k^2 \
k^3 \
k^4 \<br>
\end{array}</p>
<p>\begin{array}{cc}
q^1 &amp;
q^2 &amp;
q^3 &amp;
q^4 &amp;
\end{array}
$$</p>
<p>简写为</p>
<p>$$
A = K^T \cdot Q
$$</p>
<p><img alt="4" src="../../../img/20230209/4.png"></p>
</li>
</ol>
</li>
<li>
<p>对 $A$ 做Softmax，得到 $A^\prime$</p>
</li>
<li>
<p>$V$乘以$A^\prime$得到B</p>
<ul>
<li>$v^1$乘上 $\alpha^\prime_{1,1}$， 加上$v^2$乘上 $\alpha^\prime_{1,2}$，等等，得到 $b^1$</li>
<li>$v^1$乘上 $\alpha^\prime_{1,1}$， 加上$v^2$乘上 $\alpha^\prime_{1,2}$，等等，得到 $b^2$</li>
<li>O 就是 Output</li>
</ul>
</li>
</ol>
<p><img alt="5" src="../../../img/20230209/5.png"></p>
<p>总结：先产生QKV，根据Q找出相关的位置，再对V做weighted sum。</p>
<p><img alt="6" src="../../../img/20230209/6.png"></p>
<h2 id="怎么得到wqwk和wv">怎么得到$W^q$、$W^k$和$W^v$</h2>
<p>初始化一个矩阵，通过training data 在训练过程中更新。</p>
<h2 id="自注意模块计算两个向量的关联性">自注意模块计算两个向量的关联性</h2>
<h3 id="内积方法"><strong>内积方法</strong></h3>
<p>左边的向量乘以 $W^q$ 矩阵，右边的向量乘以 $W^k$ 矩阵，得到 $q$ 和 $k$ 这两个向量。然后做内积，得到一个标量。</p>
<p>$$
\alpha = q \cdot k
$$</p>
<h3 id="相加方法"><strong>相加方法</strong></h3>
<p>左边的向量乘以 $W^q$ 矩阵，右边的向量乘以 $W^k$ 矩阵，得到 $q$ 和 $k$ 这两个向量。然后相加丢到tanh，再乘以 $W$，得到 $\alpha$。</p>
<p><img alt="7" src="../../../img/20230209/7.png"></p>
<h2 id="参考">参考：</h2>
<p>李宏毅老师的讲课视频</p>
<p><a href="https://www.bilibili.com/video/BV1v3411r78R?p=2&vd_source=bbd38c44460fafa204a3540d4f8a2657">11.【李宏毅机器学习2021】自注意力机制 (Self-attention) (下)_哔哩哔哩_bilibili</a></p>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
</div>
    <small class="footer_copyright">
        © 2025 .
        Powered by <a href="https://github.com/hugo-sid/hugo-blog-awesome" target="_blank" rel="noopener">Hugo blog awesome</a>.
    </small>
</footer>







    
    <script async src="http://localhost:1313/js/main.js" ></script>

    

</body>
</html>
