<!doctype html><html lang=chs itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=../../../favicon.svg><title>2025-08-24 从零构建大模型-徒手组装GPT - 徐慧志的个人博客</title><meta name=description content='《从零构建大模型》8周学习计划（按周打卡！）
1️⃣ 数据预处理✅
2️⃣ 注意力机制✅
3️⃣ 徒手组装GPT✅
4️⃣ 徒手训练GPT
5️⃣ 微调：分类
6️⃣ 微调：SFT
7️⃣-8️⃣ 缓冲周
GPT的几个概念：

参数：指模型的可训练权重，本质是模型的内部变量。在训练过程中通过调整和优化来最小化特定的损失函数来学习。
GPT-2和GPT3：架构基本相同，训练的数据量不同，参数量不同。GPT-2的权重公开，GPT-3的权重没有。
GPT2的config
vocab_size表示使用50257个token组成的词汇表。

context_length指的是模型一次输入的最大token 数量。
emb_dim表示嵌入维度大小。
n_heads表示多头注意力机制中注意力头的数量。
n_layers表示模型中的Transformer块数量。
drop_rate表示表示有10%的隐藏单元被随机丢弃，以防止过拟合。
qkv_bias指的是是否在多头注意力机制的线性层中添加一个偏置向量，用于查询、键和值的计算。
GPT_CONFIG_124M ={
    "vocab_size":50257,
    "context_length":1024,
    "emb_dim":768,
    "n_heads":12,
    "n_layers":12,
    "drop_rate":0.1,
    "qkv_bias":False
}

GPT大语言模型的组件：嵌入层、Transformer块、输出层。
Transformer块包括层归一化，GELU激活函数、前馈神经网络、残差链接和多头注意力模块。


一、打好框架：构建Dummy类


这里的Dummy类只用于结构完整性，并不是具体真实的实现。


数据在模型中的处理流程：它首先计算输入索引的词元和位置嵌入，然后应用dropout，接着通过Transformer块处理数据，再应用归一化，最后使用线性输出层生成logits。


代码理解：

nn.Embedding 的本质是创建一个权重矩阵，是把词表里的每一个 token_id 映射成一个向量。
self.tok_emb = nn.Embedding(config[&ldquo;vocab_size&rdquo;], config[&ldquo;emb_dim&rdquo;])—— 在 init 里建表，告诉模型：词表有多大，每个 token 要映射成多少维的向量。这一步只发生一次，把权重矩阵（形状 [vocab_size, emb_dim]）存到 self.tok_emb 里。
tok_emb = self.tok_emb(in_idx)—— 在 forward 里查表（把输入的 token_id 拿去这张表里查对应的向量）。这一步每次前向传播都会调用，输入是形状 [batch, seq_len] 的整数张量，输出是 [batch, seq_len, emb_dim] 的嵌入张量。

import torch 
import torch.nn as nn

class DummyTransformerBlock(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
    def forward(self, x):
        return x

class DummyLayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5) -> None:
        super().__init__()
    def forward(self, x):
        return x

class DummyGPTModel(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.tok_emb = nn.Embedding(config["vocab_size"], config["emb_dim"])
        self.pos_emb = nn.Embedding(config["context_length"], config["emb_dim"])
        self.drop = nn.Dropout(config["drop_rate"])
        self.transformer_blocks = nn.Sequential(*[
            DummyTransformerBlock(config) for _ in range(config["n_layers"])
        ])  # 使用占位符替换TransformerBlock

        self.final_norm = DummyLayerNorm(config["emb_dim"])
        self.out_head = nn.Linear(config["emb_dim"], config["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_emb = self.tok_emb(in_idx)
        pos_emb = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_emb + pos_emb
        x = self.drop(x)
        x = self.transformer_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits
  
二、层归一化

层归一化指调整神经网络层的输出，使其均值为0且方差为1。
为什么要层归一化：使训练稳定，加速权重收敛。
什么时候用：多头注意力的前后；最终输出层之前
LayerNorm 沿着列方向（hidden_size） 做归一化

代码理解：'><meta name=generator content="Hugo 0.152.2"><link rel=stylesheet href="/css/styles.min.cc1204abf55b2794a944c9970dcfbbedd8cddb0c0451f7d9b088371efe0b6248.css" integrity crossorigin=anonymous><meta property="og:url" content="https://huizhixu.github.io/chs/know_how/20250824-%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%BE%92%E6%89%8B%E7%BB%84%E8%A3%85gpt/"><meta property="og:site_name" content="徐慧志的个人博客"><meta property="og:title" content="2025-08-24 从零构建大模型-徒手组装GPT"><meta property="og:description" content='《从零构建大模型》8周学习计划（按周打卡！） 1️⃣ 数据预处理✅ 2️⃣ 注意力机制✅ 3️⃣ 徒手组装GPT✅ 4️⃣ 徒手训练GPT 5️⃣ 微调：分类 6️⃣ 微调：SFT 7️⃣-8️⃣ 缓冲周
GPT的几个概念： 参数：指模型的可训练权重，本质是模型的内部变量。在训练过程中通过调整和优化来最小化特定的损失函数来学习。 GPT-2和GPT3：架构基本相同，训练的数据量不同，参数量不同。GPT-2的权重公开，GPT-3的权重没有。 GPT2的config vocab_size表示使用50257个token组成的词汇表。 context_length指的是模型一次输入的最大token 数量。
emb_dim表示嵌入维度大小。
n_heads表示多头注意力机制中注意力头的数量。
n_layers表示模型中的Transformer块数量。
drop_rate表示表示有10%的隐藏单元被随机丢弃，以防止过拟合。
qkv_bias指的是是否在多头注意力机制的线性层中添加一个偏置向量，用于查询、键和值的计算。
GPT_CONFIG_124M ={ "vocab_size":50257, "context_length":1024, "emb_dim":768, "n_heads":12, "n_layers":12, "drop_rate":0.1, "qkv_bias":False } GPT大语言模型的组件：嵌入层、Transformer块、输出层。 Transformer块包括层归一化，GELU激活函数、前馈神经网络、残差链接和多头注意力模块。 一、打好框架：构建Dummy类 这里的Dummy类只用于结构完整性，并不是具体真实的实现。
数据在模型中的处理流程：它首先计算输入索引的词元和位置嵌入，然后应用dropout，接着通过Transformer块处理数据，再应用归一化，最后使用线性输出层生成logits。
代码理解：
nn.Embedding 的本质是创建一个权重矩阵，是把词表里的每一个 token_id 映射成一个向量。 self.tok_emb = nn.Embedding(config[“vocab_size”], config[“emb_dim”])—— 在 init 里建表，告诉模型：词表有多大，每个 token 要映射成多少维的向量。这一步只发生一次，把权重矩阵（形状 [vocab_size, emb_dim]）存到 self.tok_emb 里。 tok_emb = self.tok_emb(in_idx)—— 在 forward 里查表（把输入的 token_id 拿去这张表里查对应的向量）。这一步每次前向传播都会调用，输入是形状 [batch, seq_len] 的整数张量，输出是 [batch, seq_len, emb_dim] 的嵌入张量。 import torch import torch.nn as nn class DummyTransformerBlock(nn.Module): def __init__(self, config) -> None: super().__init__() def forward(self, x): return x class DummyLayerNorm(nn.Module): def __init__(self, normalized_shape, eps=1e-5) -> None: super().__init__() def forward(self, x): return x class DummyGPTModel(nn.Module): def __init__(self, config) -> None: super().__init__() self.tok_emb = nn.Embedding(config["vocab_size"], config["emb_dim"]) self.pos_emb = nn.Embedding(config["context_length"], config["emb_dim"]) self.drop = nn.Dropout(config["drop_rate"]) self.transformer_blocks = nn.Sequential(*[ DummyTransformerBlock(config) for _ in range(config["n_layers"]) ]) # 使用占位符替换TransformerBlock self.final_norm = DummyLayerNorm(config["emb_dim"]) self.out_head = nn.Linear(config["emb_dim"], config["vocab_size"], bias=False) def forward(self, in_idx): batch_size, seq_len = in_idx.shape tok_emb = self.tok_emb(in_idx) pos_emb = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) x = tok_emb + pos_emb x = self.drop(x) x = self.transformer_blocks(x) x = self.final_norm(x) logits = self.out_head(x) return logits 二、层归一化 层归一化指调整神经网络层的输出，使其均值为0且方差为1。 为什么要层归一化：使训练稳定，加速权重收敛。 什么时候用：多头注意力的前后；最终输出层之前 LayerNorm 沿着列方向（hidden_size） 做归一化 代码理解：'><meta property="og:locale" content="chs"><meta property="og:type" content="article"><meta property="article:section" content="know_how"><meta property="article:published_time" content="2025-08-24T13:55:15+00:00"><meta property="article:modified_time" content="2025-08-24T13:55:15+00:00"><meta property="article:tag" content="Tech"><meta name=twitter:card content="summary"><meta name=twitter:title content="2025-08-24 从零构建大模型-徒手组装GPT"><meta name=twitter:description content='《从零构建大模型》8周学习计划（按周打卡！） 1️⃣ 数据预处理✅ 2️⃣ 注意力机制✅ 3️⃣ 徒手组装GPT✅ 4️⃣ 徒手训练GPT 5️⃣ 微调：分类 6️⃣ 微调：SFT 7️⃣-8️⃣ 缓冲周
GPT的几个概念： 参数：指模型的可训练权重，本质是模型的内部变量。在训练过程中通过调整和优化来最小化特定的损失函数来学习。 GPT-2和GPT3：架构基本相同，训练的数据量不同，参数量不同。GPT-2的权重公开，GPT-3的权重没有。 GPT2的config vocab_size表示使用50257个token组成的词汇表。 context_length指的是模型一次输入的最大token 数量。
emb_dim表示嵌入维度大小。
n_heads表示多头注意力机制中注意力头的数量。
n_layers表示模型中的Transformer块数量。
drop_rate表示表示有10%的隐藏单元被随机丢弃，以防止过拟合。
qkv_bias指的是是否在多头注意力机制的线性层中添加一个偏置向量，用于查询、键和值的计算。
GPT_CONFIG_124M ={ "vocab_size":50257, "context_length":1024, "emb_dim":768, "n_heads":12, "n_layers":12, "drop_rate":0.1, "qkv_bias":False } GPT大语言模型的组件：嵌入层、Transformer块、输出层。 Transformer块包括层归一化，GELU激活函数、前馈神经网络、残差链接和多头注意力模块。 一、打好框架：构建Dummy类 这里的Dummy类只用于结构完整性，并不是具体真实的实现。
数据在模型中的处理流程：它首先计算输入索引的词元和位置嵌入，然后应用dropout，接着通过Transformer块处理数据，再应用归一化，最后使用线性输出层生成logits。
代码理解：
nn.Embedding 的本质是创建一个权重矩阵，是把词表里的每一个 token_id 映射成一个向量。 self.tok_emb = nn.Embedding(config[“vocab_size”], config[“emb_dim”])—— 在 init 里建表，告诉模型：词表有多大，每个 token 要映射成多少维的向量。这一步只发生一次，把权重矩阵（形状 [vocab_size, emb_dim]）存到 self.tok_emb 里。 tok_emb = self.tok_emb(in_idx)—— 在 forward 里查表（把输入的 token_id 拿去这张表里查对应的向量）。这一步每次前向传播都会调用，输入是形状 [batch, seq_len] 的整数张量，输出是 [batch, seq_len, emb_dim] 的嵌入张量。 import torch import torch.nn as nn class DummyTransformerBlock(nn.Module): def __init__(self, config) -> None: super().__init__() def forward(self, x): return x class DummyLayerNorm(nn.Module): def __init__(self, normalized_shape, eps=1e-5) -> None: super().__init__() def forward(self, x): return x class DummyGPTModel(nn.Module): def __init__(self, config) -> None: super().__init__() self.tok_emb = nn.Embedding(config["vocab_size"], config["emb_dim"]) self.pos_emb = nn.Embedding(config["context_length"], config["emb_dim"]) self.drop = nn.Dropout(config["drop_rate"]) self.transformer_blocks = nn.Sequential(*[ DummyTransformerBlock(config) for _ in range(config["n_layers"]) ]) # 使用占位符替换TransformerBlock self.final_norm = DummyLayerNorm(config["emb_dim"]) self.out_head = nn.Linear(config["emb_dim"], config["vocab_size"], bias=False) def forward(self, in_idx): batch_size, seq_len = in_idx.shape tok_emb = self.tok_emb(in_idx) pos_emb = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) x = tok_emb + pos_emb x = self.drop(x) x = self.transformer_blocks(x) x = self.final_norm(x) logits = self.out_head(x) return logits 二、层归一化 层归一化指调整神经网络层的输出，使其均值为0且方差为1。 为什么要层归一化：使训练稳定，加速权重收敛。 什么时候用：多头注意力的前后；最终输出层之前 LayerNorm 沿着列方向（hidden_size） 做归一化 代码理解：'><meta itemprop=name content="2025-08-24 从零构建大模型-徒手组装GPT"><meta itemprop=description content='《从零构建大模型》8周学习计划（按周打卡！） 1️⃣ 数据预处理✅ 2️⃣ 注意力机制✅ 3️⃣ 徒手组装GPT✅ 4️⃣ 徒手训练GPT 5️⃣ 微调：分类 6️⃣ 微调：SFT 7️⃣-8️⃣ 缓冲周
GPT的几个概念： 参数：指模型的可训练权重，本质是模型的内部变量。在训练过程中通过调整和优化来最小化特定的损失函数来学习。 GPT-2和GPT3：架构基本相同，训练的数据量不同，参数量不同。GPT-2的权重公开，GPT-3的权重没有。 GPT2的config vocab_size表示使用50257个token组成的词汇表。 context_length指的是模型一次输入的最大token 数量。
emb_dim表示嵌入维度大小。
n_heads表示多头注意力机制中注意力头的数量。
n_layers表示模型中的Transformer块数量。
drop_rate表示表示有10%的隐藏单元被随机丢弃，以防止过拟合。
qkv_bias指的是是否在多头注意力机制的线性层中添加一个偏置向量，用于查询、键和值的计算。
GPT_CONFIG_124M ={ "vocab_size":50257, "context_length":1024, "emb_dim":768, "n_heads":12, "n_layers":12, "drop_rate":0.1, "qkv_bias":False } GPT大语言模型的组件：嵌入层、Transformer块、输出层。 Transformer块包括层归一化，GELU激活函数、前馈神经网络、残差链接和多头注意力模块。 一、打好框架：构建Dummy类 这里的Dummy类只用于结构完整性，并不是具体真实的实现。
数据在模型中的处理流程：它首先计算输入索引的词元和位置嵌入，然后应用dropout，接着通过Transformer块处理数据，再应用归一化，最后使用线性输出层生成logits。
代码理解：
nn.Embedding 的本质是创建一个权重矩阵，是把词表里的每一个 token_id 映射成一个向量。 self.tok_emb = nn.Embedding(config[“vocab_size”], config[“emb_dim”])—— 在 init 里建表，告诉模型：词表有多大，每个 token 要映射成多少维的向量。这一步只发生一次，把权重矩阵（形状 [vocab_size, emb_dim]）存到 self.tok_emb 里。 tok_emb = self.tok_emb(in_idx)—— 在 forward 里查表（把输入的 token_id 拿去这张表里查对应的向量）。这一步每次前向传播都会调用，输入是形状 [batch, seq_len] 的整数张量，输出是 [batch, seq_len, emb_dim] 的嵌入张量。 import torch import torch.nn as nn class DummyTransformerBlock(nn.Module): def __init__(self, config) -> None: super().__init__() def forward(self, x): return x class DummyLayerNorm(nn.Module): def __init__(self, normalized_shape, eps=1e-5) -> None: super().__init__() def forward(self, x): return x class DummyGPTModel(nn.Module): def __init__(self, config) -> None: super().__init__() self.tok_emb = nn.Embedding(config["vocab_size"], config["emb_dim"]) self.pos_emb = nn.Embedding(config["context_length"], config["emb_dim"]) self.drop = nn.Dropout(config["drop_rate"]) self.transformer_blocks = nn.Sequential(*[ DummyTransformerBlock(config) for _ in range(config["n_layers"]) ]) # 使用占位符替换TransformerBlock self.final_norm = DummyLayerNorm(config["emb_dim"]) self.out_head = nn.Linear(config["emb_dim"], config["vocab_size"], bias=False) def forward(self, in_idx): batch_size, seq_len = in_idx.shape tok_emb = self.tok_emb(in_idx) pos_emb = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) x = tok_emb + pos_emb x = self.drop(x) x = self.transformer_blocks(x) x = self.final_norm(x) logits = self.out_head(x) return logits 二、层归一化 层归一化指调整神经网络层的输出，使其均值为0且方差为1。 为什么要层归一化：使训练稳定，加速权重收敛。 什么时候用：多头注意力的前后；最终输出层之前 LayerNorm 沿着列方向（hidden_size） 做归一化 代码理解：'><meta itemprop=datePublished content="2025-08-24T13:55:15+00:00"><meta itemprop=dateModified content="2025-08-24T13:55:15+00:00"><meta itemprop=wordCount content="790"><meta itemprop=keywords content="Tech"><meta name=lang content="chs"></head><body class="dark:bg-gray-800 dark:text-white relative flex flex-col min-h-screen"><header class="container flex justify-between md:justify-between gap-4 flex-wrap p-6 mx-auto relative"><a href=https://huizhixu.github.io/chs/ class="capitalize font-extrabold text-2xl"><img src=../../../blist-logo.png alt=徐慧志的个人博客 class="h-8 max-w-full">
</a><button class="mobile-menu-button md:hidden">
<svg width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="4" y1="8" x2="20" y2="8"/><line x1="4" y1="16" x2="20" y2="16"/></svg></button><ul class="mobile-menu absolute z-10 px-6 pb-6 md:p-0 top-full left-0 w-full md:w-auto md:relative hidden md:flex flex-col md:flex-row items-end md:items-center gap-4 lg:gap-6 bg-white dark:bg-gray-800"><li><a href=../../../chs/know_how/>技术</a></li><li><a href=../../../chs/life/>生活见闻</a></li><li><a href=../../../chs/page/about/>关于</a></li><li><a href=../../../chs/link/>宝藏集结</a></li><li><a href=../../../chs/tags/>分类</a></li><li class="relative cursor-pointer"><span class="language-switcher flex items-center gap-2"><svg width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><line x1="3.6" y1="9" x2="20.4" y2="9"/><line x1="3.6" y1="15" x2="20.4" y2="15"/><path d="M11.5 3a17 17 0 000 18"/><path d="M12.5 3a17 17 0 010 18"/></svg>
<a>语言</a>
<svg width="14" height="14" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 15l-6-6-6 6h12" transform="rotate(180 12 12)"/></svg></span><div class="language-dropdown absolute top-full mt-2 left-0 flex-col gap-2 bg-gray-100 dark:bg-gray-900 dark:text-white z-10 hidden"><a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href=../../../en/ lang=en>English</a>
<a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href=../../../de/ lang=de>Deutsch</a></div></li><li class="grid place-items-center"><span class="open-search inline-block cursor-pointer"><svg width="20" height="20" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></span></li><li class="grid place-items-center"><span class="toggle-dark-mode inline-block cursor-pointer"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="3"/><line x1="12" y1="5" x2="12" y2="5.01"/><line x1="17" y1="7" x2="17" y2="7.01"/><line x1="19" y1="12" x2="19" y2="12.01"/><line x1="17" y1="17" x2="17" y2="17.01"/><line x1="12" y1="19" x2="12" y2="19.01"/><line x1="7" y1="17" x2="7" y2="17.01"/><line x1="5" y1="12" x2="5" y2="12.01"/><line x1="7" y1="7" x2="7" y2="7.01"/></svg></span></li></ul></header><main class=flex-1><article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4"><h1 class="text-2xl font-bold mb-2">2025-08-24 从零构建大模型-徒手组装GPT</h1><h5 class="text-sm flex items-center flex-wrap"><svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg>
发布于
2025年08月24日
&nbsp;&bull;&nbsp;
<svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
4&nbsp;分钟
&nbsp;&bull;
<svg class="mx-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 19a9 9 0 019 0 9 9 0 019 0"/><path d="M3 6a9 9 0 019 0 9 9 0 019 0"/><line x1="3" y1="6" x2="3" y2="19"/><line x1="12" y1="6" x2="12" y2="19"/><line x1="21" y1="6" x2="21" y2="19"/></svg>
790&nbsp;字</h5><details id=TableOfContents class="px-4 mt-4 bg-gray-100 dark:bg-gray-700 rounded toc"><summary class="flex items-center font-bold py-2 px-4 cursor-pointer justify-between select-none text-black dark:text-white"><span>Table of contents</span>
<svg class="icon icon-tabler icon-tabler-chevron-down" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="6 9 12 15 18 9"/></svg></summary><ul class="mt-2 pb-4"><li><a href=#gpt%e7%9a%84%e5%87%a0%e4%b8%aa%e6%a6%82%e5%bf%b5>GPT的几个概念：</a></li><li><a href=#%e4%b8%80%e6%89%93%e5%a5%bd%e6%a1%86%e6%9e%b6%e6%9e%84%e5%bb%badummy%e7%b1%bb>一、打好框架：构建Dummy类</a></li><li><a href=#%e4%ba%8c%e5%b1%82%e5%bd%92%e4%b8%80%e5%8c%96>二、层归一化</a></li><li><a href=#%e4%b8%89gelu%e5%87%bd%e6%95%b0%e5%92%8c%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c>三、GELU函数和前馈神经网络</a></li><li><a href=#%e5%9b%9b%e5%bf%ab%e6%8d%b7%e8%bf%9e%e6%8e%a5>四、快捷连接</a></li><li><a href=#%e4%ba%94transformer%e5%9d%97>五、Transformer块</a></li><li><a href=#%e5%85%adgpt%e6%9e%b6%e6%9e%84%e7%9a%84%e7%9c%9f%e6%ad%a3%e5%ae%9e%e7%8e%b0>六、GPT架构的真正实现</a></li><li><a href=#%e4%b8%83gpt%e5%b0%86%e8%be%93%e5%87%ba%e5%bc%a0%e9%87%8f%e8%bd%ac%e4%b8%ba%e6%96%87%e6%9c%ac>七、GPT将输出张量转为文本</a></li><li><a href=#%e5%85%ab%e4%ba%86%e8%a7%a3%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e5%8f%8a%e5%85%b6%e8%a7%84%e6%a8%a1>八、了解模型架构及其规模</a></li></ul></details><p>《从零构建大模型》8周学习计划（按周打卡！）
1️⃣ 数据预处理✅
2️⃣ 注意力机制✅
3️⃣ 徒手组装GPT✅
4️⃣ 徒手训练GPT
5️⃣ 微调：分类
6️⃣ 微调：SFT
7️⃣-8️⃣ 缓冲周</p><h2 id=gpt的几个概念>GPT的几个概念：</h2><ol><li>参数：指模型的可训练权重，本质是模型的内部变量。在训练过程中通过调整和优化来最小化特定的损失函数来学习。</li><li>GPT-2和GPT3：架构基本相同，训练的数据量不同，参数量不同。GPT-2的权重公开，GPT-3的权重没有。</li><li>GPT2的config
vocab_size表示使用50257个token组成的词汇表。</li></ol><p>context_length指的是模型一次输入的最大token 数量。</p><p>emb_dim表示嵌入维度大小。</p><p>n_heads表示多头注意力机制中注意力头的数量。</p><p>n_layers表示模型中的Transformer块数量。</p><p>drop_rate表示表示有10%的隐藏单元被随机丢弃，以防止过拟合。</p><p>qkv_bias指的是是否在多头注意力机制的线性层中添加一个偏置向量，用于查询、键和值的计算。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>GPT_CONFIG_124M <span style=color:#ff79c6>=</span>{
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;vocab_size&#34;</span>:<span style=color:#bd93f9>50257</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;context_length&#34;</span>:<span style=color:#bd93f9>1024</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;emb_dim&#34;</span>:<span style=color:#bd93f9>768</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;n_heads&#34;</span>:<span style=color:#bd93f9>12</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;n_layers&#34;</span>:<span style=color:#bd93f9>12</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;drop_rate&#34;</span>:<span style=color:#bd93f9>0.1</span>,
</span></span><span style=display:flex><span>    <span style=color:#f1fa8c>&#34;qkv_bias&#34;</span>:<span style=color:#ff79c6>False</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ol><li>GPT大语言模型的组件：嵌入层、Transformer块、输出层。</li><li>Transformer块包括层归一化，GELU激活函数、前馈神经网络、残差链接和多头注意力模块。</li></ol><p><img src=https://raw.githubusercontent.com/HuizhiXu/pictures/master/20250824/668b362d.png alt></p><h2 id=一打好框架构建dummy类>一、打好框架：构建Dummy类</h2><ol><li><p>这里的Dummy类只用于结构完整性，并不是具体真实的实现。</p></li><li><p>数据在模型中的处理流程：它首先计算输入索引的词元和位置嵌入，然后应用dropout，接着通过Transformer块处理数据，再应用归一化，最后使用线性输出层生成logits。</p></li></ol><p>代码理解：</p><ol><li>nn.Embedding 的本质是创建一个权重矩阵，是把词表里的每一个 token_id 映射成一个向量。</li><li>self.tok_emb = nn.Embedding(config[&ldquo;vocab_size&rdquo;], config[&ldquo;emb_dim&rdquo;])—— 在 <strong>init</strong> 里建表，告诉模型：词表有多大，每个 token 要映射成多少维的向量。这一步只发生一次，把权重矩阵（形状 [vocab_size, emb_dim]）存到 self.tok_emb 里。</li><li>tok_emb = self.tok_emb(in_idx)—— 在 forward 里查表（把输入的 token_id 拿去这张表里查对应的向量）。这一步每次前向传播都会调用，输入是形状 [batch, seq_len] 的整数张量，输出是 [batch, seq_len, emb_dim] 的嵌入张量。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>import</span> torch 
</span></span><span style=display:flex><span><span style=color:#ff79c6>import</span> torch.nn <span style=color:#ff79c6>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>DummyTransformerBlock</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, config) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>, x):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>DummyLayerNorm</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, normalized_shape, eps<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1e-5</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>, x):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>DummyGPTModel</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, config) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>tok_emb <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Embedding(config[<span style=color:#f1fa8c>&#34;vocab_size&#34;</span>], config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_emb <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Embedding(config[<span style=color:#f1fa8c>&#34;context_length&#34;</span>], config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>drop <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Dropout(config[<span style=color:#f1fa8c>&#34;drop_rate&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>transformer_blocks <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Sequential(<span style=color:#ff79c6>*</span>[
</span></span><span style=display:flex><span>            DummyTransformerBlock(config) <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(config[<span style=color:#f1fa8c>&#34;n_layers&#34;</span>])
</span></span><span style=display:flex><span>        ])  <span style=color:#6272a4># 使用占位符替换TransformerBlock</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>final_norm <span style=color:#ff79c6>=</span> DummyLayerNorm(config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>out_head <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Linear(config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>], config[<span style=color:#f1fa8c>&#34;vocab_size&#34;</span>], bias<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>, in_idx):
</span></span><span style=display:flex><span>        batch_size, seq_len <span style=color:#ff79c6>=</span> in_idx<span style=color:#ff79c6>.</span>shape
</span></span><span style=display:flex><span>        tok_emb <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>tok_emb(in_idx)
</span></span><span style=display:flex><span>        pos_emb <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_emb(torch<span style=color:#ff79c6>.</span>arange(seq_len, device<span style=color:#ff79c6>=</span>in_idx<span style=color:#ff79c6>.</span>device))
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> tok_emb <span style=color:#ff79c6>+</span> pos_emb
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>drop(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>transformer_blocks(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>final_norm(x)
</span></span><span style=display:flex><span>        logits <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>out_head(x)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> logits
</span></span><span style=display:flex><span>  
</span></span></code></pre></div><h2 id=二层归一化>二、层归一化</h2><ol><li>层归一化指调整神经网络层的输出，使其均值为0且方差为1。</li><li>为什么要层归一化：使训练稳定，加速权重收敛。</li><li>什么时候用：多头注意力的前后；最终输出层之前</li><li>LayerNorm 沿着列方向（hidden_size） 做归一化</li></ol><p>代码理解：</p><ol><li>变量eps是一个小常数，在归一化过程中会被加到方差上以防止除零错误。</li><li>scale和shift是两个可训练的参数（与输入维度相同），训练过程中大语言模型会自动调整</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>LayerNorm</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, emb_dim):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>eps <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>1e-5</span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>scale <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Parameter(torch<span style=color:#ff79c6>.</span>ones(emb_dim))
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>shift <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Parameter(torch<span style=color:#ff79c6>.</span>zeros(emb_dim))
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>,x):
</span></span><span style=display:flex><span>        mean <span style=color:#ff79c6>=</span> x<span style=color:#ff79c6>.</span>mean(dim<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>, keepdim<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>        var <span style=color:#ff79c6>=</span> x<span style=color:#ff79c6>.</span>var(dim<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>, keepdim<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>        norm_x <span style=color:#ff79c6>=</span> (x <span style=color:#ff79c6>-</span> mean) <span style=color:#ff79c6>/</span> torch<span style=color:#ff79c6>.</span>sqrt(var <span style=color:#ff79c6>+</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>eps)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>scale <span style=color:#ff79c6>*</span> norm_x <span style=color:#ff79c6>+</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>shift
</span></span></code></pre></div><h2 id=三gelu函数和前馈神经网络>三、GELU函数和前馈神经网络</h2><p>GELU(Gaussian Error Linear Unit) 其精确的定义为GELU(x) = x \cdot \Phi(x)，其中\Phi(x)是标准高斯分布的累积分布函数。</p><p>GELU的好处：</p><ol><li>有平滑性，可以在训练过程中带来更好的优化效果</li><li>对负输入会输出一个小的非零值。这意味着在训练过程中，接收到负输入的神经元仍然可以参与学习</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>GELU</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>,x):
</span></span><span style=display:flex><span>		    <span style=color:#6272a4># 这个是近似实现</span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=color:#bd93f9>0.5</span> <span style=color:#ff79c6>*</span> x <span style=color:#ff79c6>*</span> (<span style=color:#bd93f9>1</span> <span style=color:#ff79c6>+</span> torch<span style=color:#ff79c6>.</span>tanh(torch<span style=color:#ff79c6>.</span>sqrt(torch<span style=color:#ff79c6>.</span>tensor(<span style=color:#bd93f9>2.0</span> <span style=color:#ff79c6>/</span> torch<span style=color:#ff79c6>.</span>pi)) <span style=color:#ff79c6>*</span> (x <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>0.044715</span> <span style=color:#ff79c6>*</span> torch<span style=color:#ff79c6>.</span>pow(x, <span style=color:#bd93f9>3</span>))))
</span></span></code></pre></div><p>前馈神经网络由两个线性层和一个GELU激活函数组成。</p><p>这个网络的输入和输出维度保持一致，但它通过第一个线性层将嵌入维度扩展到了更高的维度（代码这里是4倍输入的维度）。扩展之后，应用非线性GELU激活函数，然后通过第二个线性变换将维度缩回原始大小。这种设计允许模型探索更丰富的表示空间。</p><p>为什么是4倍呢？</p><p>一是早期 Transformer 论文设置的4倍， 后续所有实现为了可复现，就沿用了同一比例。二是4倍在工程上能够平衡容量和计算量，被认为是一个比较好的考虑。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>FeedForward</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>,config):
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layers <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Sequential(
</span></span><span style=display:flex><span>            nn<span style=color:#ff79c6>.</span>Linear(config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>], <span style=color:#bd93f9>4</span> <span style=color:#ff79c6>*</span> config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>]),
</span></span><span style=display:flex><span>            GELU(),
</span></span><span style=display:flex><span>            nn<span style=color:#ff79c6>.</span>Linear(<span style=color:#bd93f9>4</span> <span style=color:#ff79c6>*</span> config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>], config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>]),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>,x):
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>layers(x)
</span></span></code></pre></div><h2 id=四快捷连接>四、快捷连接</h2><p>快捷连接就是残差连接，这里为了和书上保持一致，就继续用“快捷链接”这个名字了。</p><p>快捷连接通过跳过一个或多个层，为梯度在网络中的流动提供了一条可替代且更短的路径。它让梯度可以直接反向传播，有效解决了深层网络中的梯度消失问题。</p><p>x&rsquo; = x + Attention(LayerNorm(x))</p><p><img src=https://raw.githubusercontent.com/HuizhiXu/pictures/master/20250824/14a79927.png alt></p><h2 id=五transformer块>五、Transformer块</h2><p>Transformer块包括层归一化，GELU激活函数、前馈神经网络、残差链接和多头注意力模块。</p><p>当Transformer块处理输入序列时，序列中的每个token都被表示为一个固定大小的向量（此处为768维）。</p><ol><li>先对输入做层归一化，再进入多头注意力层，捕捉 token 间关系，随后用残差连接把结果加回原向量；</li><li>再做一次层归一化，送入逐位置前馈网络（先扩维再缩维，中间用 GELU），同样用残差连接；</li><li>两处子层内部都配有 Dropout 防止过拟合。层归一化稳定分布，残差连接缓解梯度消失并保留恒等路径。
<img src=https://raw.githubusercontent.com/HuizhiXu/pictures/master/20250824/c3f03483.png alt></li></ol><p>图源于《从零构建大模型》</p><p>代码解释：</p><ol><li>前层归一化（Pre-LayerNorm）和后层归一化(Post-LayerNorm)</li><li>Transformer块让输入和输出保持不变，为什么？
这种设计使其能有效应用于各种序列到序列的任务，其中每个输出向量直接对应一个输入向量，保持一一对应的关系。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>from</span> attention_mechanism.attention_module <span style=color:#ff79c6>import</span> MultiHeadAttention
</span></span><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>TransformerBlock</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, config) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>attention <span style=color:#ff79c6>=</span> MultiHeadAttention(
</span></span><span style=display:flex><span>            d_in <span style=color:#ff79c6>=</span> config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>],
</span></span><span style=display:flex><span>            d_out <span style=color:#ff79c6>=</span> config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>],
</span></span><span style=display:flex><span>            context_length <span style=color:#ff79c6>=</span> config[<span style=color:#f1fa8c>&#34;context_length&#34;</span>],
</span></span><span style=display:flex><span>            dropout <span style=color:#ff79c6>=</span> config[<span style=color:#f1fa8c>&#34;drop_rate&#34;</span>],
</span></span><span style=display:flex><span>            num_heads <span style=color:#ff79c6>=</span> config[<span style=color:#f1fa8c>&#34;n_heads&#34;</span>],
</span></span><span style=display:flex><span>            qkv_bias <span style=color:#ff79c6>=</span> config[<span style=color:#f1fa8c>&#34;qkv_bias&#34;</span>]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>ff <span style=color:#ff79c6>=</span> FeedForward(config)
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>norm1 <span style=color:#ff79c6>=</span> LayerNorm(config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>norm2 <span style=color:#ff79c6>=</span> LayerNorm(config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Dropout(config[<span style=color:#f1fa8c>&#34;drop_rate&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>, x):
</span></span><span style=display:flex><span>        shortcut <span style=color:#ff79c6>=</span> x
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>norm1(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>attention(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout(x) 
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> x <span style=color:#ff79c6>+</span> shortcut
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        shortcut <span style=color:#ff79c6>=</span> x
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>norm2(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>ff(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>dropout(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> x <span style=color:#ff79c6>+</span> shortcut
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> x
</span></span></code></pre></div><h2 id=六gpt架构的真正实现>六、GPT架构的真正实现</h2><p>GPT大语言模型的组件包括嵌入层、Transformer块和输出层。</p><p>嵌入层：输入序列首先被转换成词元嵌入，然后用位置嵌入进行增强。</p><p>Transformer块：通过一系列Transformer块（每个块都包含多头注意力和前馈神经网络层，并带有dropout和层归一化功能），这些块相互堆叠并重复12次最终Transformer块的输出会经过最后一步的层归一化处理，然后传递到线性输出层。</p><p>输出层：将Transformer的输出映射到一个高维空间（大小为对应模型的词汇表大小），以预测序列中的下一个词元。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>class</span> <span style=color:#50fa7b>GPTModel</span>(nn<span style=color:#ff79c6>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>__init__</span>(<span style=font-style:italic>self</span>, config) <span style=color:#ff79c6>-&gt;</span> <span style=color:#ff79c6>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#8be9fd;font-style:italic>super</span>()<span style=color:#ff79c6>.</span><span style=color:#50fa7b>__init__</span>()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>tok_emb <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Embedding(config[<span style=color:#f1fa8c>&#34;vocab_size&#34;</span>], config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_emb <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Embedding(config[<span style=color:#f1fa8c>&#34;context_length&#34;</span>], config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>drop <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Dropout(config[<span style=color:#f1fa8c>&#34;drop_rate&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>transformer_blocks <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Sequential(<span style=color:#ff79c6>*</span>[
</span></span><span style=display:flex><span>            TransformerBlock(config) <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(config[<span style=color:#f1fa8c>&#34;n_layers&#34;</span>])
</span></span><span style=display:flex><span>        ])  <span style=color:#6272a4># 使用占位符替换TransformerBlock</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>final_norm <span style=color:#ff79c6>=</span> LayerNorm(config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>])
</span></span><span style=display:flex><span>        <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>out_head <span style=color:#ff79c6>=</span> nn<span style=color:#ff79c6>.</span>Linear(config[<span style=color:#f1fa8c>&#34;emb_dim&#34;</span>], config[<span style=color:#f1fa8c>&#34;vocab_size&#34;</span>], bias<span style=color:#ff79c6>=</span><span style=color:#ff79c6>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>def</span> <span style=color:#50fa7b>forward</span>(<span style=font-style:italic>self</span>, in_idx):
</span></span><span style=display:flex><span>        batch_size, seq_len <span style=color:#ff79c6>=</span> in_idx<span style=color:#ff79c6>.</span>shape
</span></span><span style=display:flex><span>        tok_emb <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>tok_emb(in_idx)
</span></span><span style=display:flex><span>        pos_emb <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>pos_emb(torch<span style=color:#ff79c6>.</span>arange(seq_len, device<span style=color:#ff79c6>=</span>in_idx<span style=color:#ff79c6>.</span>device))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> tok_emb <span style=color:#ff79c6>+</span> pos_emb
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>drop(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>transformer_blocks(x)
</span></span><span style=display:flex><span>        x <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>final_norm(x)
</span></span><span style=display:flex><span>        logits <span style=color:#ff79c6>=</span> <span style=font-style:italic>self</span><span style=color:#ff79c6>.</span>out_head(x)
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>return</span> logits
</span></span></code></pre></div><h2 id=七gpt将输出张量转为文本>七、GPT将输出张量转为文本</h2><ol><li><p>做法：模型在每轮迭代中预测下一个词元，并将其添加到输入上下文中以进行下一轮预测</p></li><li><p>步骤：</p></li><li><p>细节：
在每一步中，模型输出一个矩阵，其中的向量表示有可能的下一个词元。将与下一个词元对应的向量提取出来，并通过softmax函数转换为概率分布。在包含这些概率分数的向量中，找到最高值的索引，这个索引对应于词元ID。然后将这个词元ID解码为文本，生成序列中的下一个词元。最后，将这个词元附加到之前的输入中，形成新的输入序列，供下一次迭代使用。这个逐步的过程使得模型能够按顺序生成文本，从最初的输入上下文中构建连贯的短语和句子。</p></li></ol><p>代码解读</p><ol><li><p>贪心解码
每一步都只选概率最高的词模型输出一个词汇表大小的概率分布后，直接取 argmax 得到当前最可能的 token，然后把它作为下一步的输入，再继续预测，直到遇到结束符或达到最大长度。</p></li><li><p>idx[:, -context_size:] 的作用是：只保留每个样本（batch 中的每一行）最后的 context_size 个 token。</p></li><li><p>-context_size: 表示 从倒数第 context_size 个元素开始，一直取到末尾。</p></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#ff79c6>def</span> <span style=color:#50fa7b>generate_text_simple</span>(model, idx, max_new_tokens, context_size):
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>for</span> _ <span style=color:#ff79c6>in</span> <span style=color:#8be9fd;font-style:italic>range</span>(max_new_tokens):
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 将当前文本截断至大模型支持的长度</span>
</span></span><span style=display:flex><span>        idx_cond <span style=color:#ff79c6>=</span> idx[:, <span style=color:#ff79c6>-</span>context_size:]
</span></span><span style=display:flex><span>        <span style=color:#ff79c6>with</span> torch<span style=color:#ff79c6>.</span>no_grad():
</span></span><span style=display:flex><span>            logits <span style=color:#ff79c6>=</span> model(idx_cond)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 仅关注最后一个时间步的logits</span>
</span></span><span style=display:flex><span>        logits <span style=color:#ff79c6>=</span> logits[:, <span style=color:#ff79c6>-</span><span style=color:#bd93f9>1</span>, :]
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 转换为概率分布</span>
</span></span><span style=display:flex><span>        probs <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>softmax(logits, dim<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 采样下一个token</span>
</span></span><span style=display:flex><span>        idx_next <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>argmax(probs, dim<span style=color:#ff79c6>=-</span><span style=color:#bd93f9>1</span>, keepdim<span style=color:#ff79c6>=</span><span style=color:#ff79c6>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#6272a4># 拼接采样的token</span>
</span></span><span style=display:flex><span>        idx <span style=color:#ff79c6>=</span> torch<span style=color:#ff79c6>.</span>cat((idx, idx_next), dim<span style=color:#ff79c6>=</span><span style=color:#bd93f9>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#ff79c6>return</span> idx
</span></span></code></pre></div><p>尝试输入 &ldquo;Hello, I am a robot.”</p><p>输出结果 为 ”Hello, I am a robot.mail unheard lawn nan Mac bridinates spring denotesnosis” 。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#ff79c6>.</span>eval()
</span></span><span style=display:flex><span>    out <span style=color:#ff79c6>=</span> generate_text_simple(
</span></span><span style=display:flex><span>        model, 
</span></span><span style=display:flex><span>        batch, 
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#ff79c6>=</span><span style=color:#bd93f9>10</span>, 
</span></span><span style=display:flex><span>        context_size<span style=color:#ff79c6>=</span>GPT_CONFIG_124M[<span style=color:#f1fa8c>&#34;context_length&#34;</span>])
</span></span></code></pre></div><p>到目前为止，我们只是实现了GPT架构，并用初始随机权重初始化了GPT模型实例，并没有对其训练，所以输出的是无意义的文本。</p><h2 id=八了解模型架构及其规模>八、了解模型架构及其规模</h2><ol><li>权重共享</li></ol><p>用文章最前面初始化的GPT_CONFIG_124M（1.24亿）初始化模型，并计算构造的GPT的参数量，发现实际的参数量是1.63亿。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#6272a4># 通过numel()（“number of elements”的缩写）方法统计模型参数张量的总参数量</span>
</span></span><span style=display:flex><span>model <span style=color:#ff79c6>=</span> GPTModel(GPT_CONFIG_124M)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>total_params <span style=color:#ff79c6>=</span> <span style=color:#8be9fd;font-style:italic>sum</span>(p<span style=color:#ff79c6>.</span>numel() <span style=color:#ff79c6>for</span> p <span style=color:#ff79c6>in</span> model<span style=color:#ff79c6>.</span>parameters())
</span></span><span style=display:flex><span>    <span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Total number of parameters: </span><span style=color:#f1fa8c>{</span>total_params<span style=color:#f1fa8c>:</span><span style=color:#f1fa8c>,</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#6272a4># Total number of parameters: 163,009,536</span>
</span></span></code></pre></div><p>那这个差距是怎么来的呢？</p><p>因为原始的GPT-2的架构中使用了“权重共享”——输出层的权重矩阵直接复用词元嵌入层的权重。</p><p>词元嵌入层和线性输出层的权重具有相同的形状，都是[50257, 768]（总参数量词表大小 V = 50 257，嵌入层维度 d = 768）。这两层的大小为50 257 × 768 ≈ 38.6 M。</p><p>1.63亿- 0.386亿= 1.24亿。</p><ol><li>前馈模块和注意力模块包含的参数量</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>已知 GPT<span style=color:#ff79c6>-</span><span style=color:#bd93f9>2</span> (small) 超参
</span></span><span style=display:flex><span>d_model <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>768</span>
</span></span><span style=display:flex><span>d_ff <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span> × d_model <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3072</span>
</span></span><span style=display:flex><span>n_head <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>12</span>
</span></span><span style=display:flex><span>vocab_size <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>50257</span>（只影响嵌入<span style=color:#ff79c6>/</span>输出层，不影响 block 内部）
</span></span></code></pre></div><p>FeedForward: 4,722,432</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>FeedForward（两层全连接)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>权重矩阵形状
</span></span><span style=display:flex><span>W₁: d_model × d_ff <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>768</span> × <span style=color:#bd93f9>3072</span>
</span></span><span style=display:flex><span>b₁: <span style=color:#bd93f9>3072</span>
</span></span><span style=display:flex><span>W₂: d_ff × d_model <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3072</span> × <span style=color:#bd93f9>768</span>
</span></span><span style=display:flex><span>b₂: <span style=color:#bd93f9>768</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>参数量 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>768</span>×<span style=color:#bd93f9>3072</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>3072</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>3072</span>×<span style=color:#bd93f9>768</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>768</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>×(<span style=color:#bd93f9>768</span>×<span style=color:#bd93f9>3072</span>) <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>3072</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>768</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span>,<span style=color:#bd93f9>718</span>,<span style=color:#bd93f9>592</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>3</span>,<span style=color:#bd93f9>072</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>768</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span>,<span style=color:#bd93f9>722</span>,<span style=color:#bd93f9>432</span> 
</span></span></code></pre></div><p>MultiHeadAttention：2,361,600</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>MultiHeadAttention（含 <span style=color:#bd93f9>4</span> 个线性映射：Q, K, V, O）
</span></span><span style=display:flex><span>每个 head 的 d_head <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>768</span> <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>12</span> <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>64</span>
</span></span><span style=display:flex><span>四个权重矩阵形状都是 d_model × d_model <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>768</span> × <span style=color:#bd93f9>768</span>，没有额外偏置。
</span></span><span style=display:flex><span>参数量 <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span> × (<span style=color:#bd93f9>768</span>×<span style=color:#bd93f9>768</span>)
</span></span><span style=display:flex><span><span style=color:#ff79c6>=</span> <span style=color:#bd93f9>4</span> × <span style=color:#bd93f9>589</span>,<span style=color:#bd93f9>824</span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,<span style=color:#bd93f9>359</span>,<span style=color:#bd93f9>296</span>
</span></span><span style=display:flex><span>但 PyTorch 的 nn<span style=color:#ff79c6>.</span>MultiheadAttention 实现里默认 给 QKV 加偏置，而 O 投影无偏置：
</span></span><span style=display:flex><span><span style=color:#bd93f9>3</span> 个偏置 (<span style=color:#bd93f9>768</span> 维) <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>3</span> × <span style=color:#bd93f9>768</span> <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,<span style=color:#bd93f9>304</span>
</span></span><span style=display:flex><span>因此实际计算常看到 <span style=color:#bd93f9>2</span>,<span style=color:#bd93f9>359</span>,<span style=color:#bd93f9>296</span> <span style=color:#ff79c6>+</span> <span style=color:#bd93f9>2</span>,<span style=color:#bd93f9>304</span> <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>2</span>,<span style=color:#bd93f9>361</span>,<span style=color:#bd93f9>600</span>。
</span></span></code></pre></div><ol><li>内存需求
GPTModel对象中1.63亿个参数的内存需求，并假设每个参数是占用4字节的32位浮点数。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>total_size_bytes <span style=color:#ff79c6>=</span> total_params <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>4</span>  <span style=color:#6272a4># 32微浮点数</span>
</span></span><span style=display:flex><span>total_size_mb <span style=color:#ff79c6>=</span> total_size_bytes <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>1024</span> <span style=color:#ff79c6>/</span> <span style=color:#bd93f9>1024</span>  <span style=color:#6272a4># 转为兆字节</span>
</span></span><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>print</span>(<span style=color:#f1fa8c>f</span><span style=color:#f1fa8c>&#34;Total size: </span><span style=color:#f1fa8c>{</span>total_size_mb<span style=color:#f1fa8c>:</span><span style=color:#f1fa8c>.2f</span><span style=color:#f1fa8c>}</span><span style=color:#f1fa8c> MB&#34;</span>)
</span></span></code></pre></div><p>模型需要的内存空间为621.83 MB。</p></article><div class="px-2 mb-2"><script src=https://utteranc.es/client.js repo=HuizhiXu/huizhixu.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div><div class="bg-blue-100 dark:bg-gray-900"><div class="container px-4 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-4 items-center"><div><div class="text-2xl font-bold mb-2">Sein heißt werden, leben heißt lernen.</div><p class=opacity-60>Der einfache Weg is immer verkehrt.</p></div><ul class="flex justify-center gap-x-3 flex-wrap gap-y-2"><li><a href=https://twitter.com/ target=_blank rel=noopener aria-label=Twitter class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://github.com/ target=_blank rel=noopener aria-label=GitHub class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ul></div></div></main><footer class="container p-6 mx-auto flex justify-between items-center"><span class="text-sm font-light">Copyright © 2012 - Huizhi Xu · All rights reserved
</span><span onclick='window.scrollTo({top:0,behavior:"smooth"})' class="p-1 cursor-pointer"><svg width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 15l-6-6-6 6h12"/></svg></span></footer><div class="search-ui absolute top-0 left-0 w-full h-full bg-white dark:bg-gray-800 hidden"><div class="container max-w-3xl mx-auto p-12"><div class=relative><div class="my-4 text-center text-2xl font-bold">Search</div><span class="p-2 absolute right-0 top-0 cursor-pointer close-search"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></span></div><input type=search class="py-2 px-3 w-full dark:text-black border dark:border-transparent" placeholder="Enter search query"><div class="search-results text-lg font-medium my-4 hidden">Results</div><ul class="search-list my-2"></ul><div class="no-results text-center my-8 hidden"><div class="text-xl font-semibold mb-2">No results found</div><p class="font-light text-sm">Try adjusting your search query</p></div></div></div><script src=https://huizhixu.github.io/js/scripts.min.js></script><script>const languageMenuButton=document.querySelector(".language-switcher"),languageDropdown=document.querySelector(".language-dropdown");languageMenuButton.addEventListener("click",e=>{e.preventDefault(),languageDropdown.classList.contains("hidden")?(languageDropdown.classList.remove("hidden"),languageDropdown.classList.add("flex")):(languageDropdown.classList.add("hidden"),languageDropdown.classList.remove("flex"))})</script><script>const darkmode=document.querySelector(".toggle-dark-mode");function toggleDarkMode(){document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("darkmode","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("darkmode","dark"))}darkmode&&darkmode.addEventListener("click",toggleDarkMode);const darkStorage=localStorage.getItem("darkmode"),isBrowserDark=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches;!darkStorage&&isBrowserDark&&document.documentElement.classList.add("dark"),darkStorage&&darkStorage==="dark"&&toggleDarkMode()</script><script>const mobileMenuButton=document.querySelector(".mobile-menu-button"),mobileMenu=document.querySelector(".mobile-menu");function toggleMenu(){mobileMenu.classList.toggle("hidden"),mobileMenu.classList.toggle("flex")}mobileMenu&&mobileMenuButton&&mobileMenuButton.addEventListener("click",toggleMenu)</script></body></html>