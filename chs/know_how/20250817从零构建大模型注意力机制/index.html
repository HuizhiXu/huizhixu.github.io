<!DOCTYPE html>
<html lang="chs" itemscope itemtype="http://schema.org/WebPage"><head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="../../../favicon.svg">

  <title>
  2025-08-17 从零构建大模型——注意力机制 - 徐慧志的个人博客
  </title>
  <meta name="description" content="背景
编码器-解码器
编码器将源语言的一串词元序列作为输入，并通过隐藏状态（一个中间神经网络层）编码整个输入序列的压缩表示（可以理解为嵌入）。然后，解码器利用其当前的隐藏状态开始逐个词元进行解码生成。
编码器-解码器RNN的缺陷：在解码阶段，RNN无法直接访问编码器中的早期隐藏状态，它只能依赖当前的隐藏状态。这可能导致上下文丢失，特别是在依赖关系可能跨越较长的距离的句子中。
构建大语言模型的三个阶段

（图来源于书籍）
这张图画得很清晰，第三章的主要学习注意力机制。
学习目标
实现4种注意力机制

简化版的自注意力机制
加入可训练的权重的自注意力机制
因果注意力机制
多头注意力机制

简化版的自注意力机制
关键概念

注意力机制：对于输出，某些输入词元比其他词元更重要。重要性由注意力权重决定。
“自”是什么意思：
上下文向量是什么？
上下文向量怎么计算？

实践
import torch
inputs = torch.tensor(
  [[0.43, 0.15, 0.89], # Your     (x^1)
   [0.55, 0.87, 0.66], # journey  (x^2)
   [0.57, 0.85, 0.64], # starts   (x^3)
   [0.22, 0.58, 0.33], # with     (x^4)
   [0.77, 0.25, 0.10], # one      (x^5)
   [0.05, 0.80, 0.55]] # step     (x^6)
)

print(inputs.shape[0])
query = inputs[1]
attn_scores = torch.empty(inputs.shape[0],inputs.shape[0])
attn_scores = inputs @ inputs.T
print(attn_scores)

attn_weights = torch.softmax(attn_scores, dim=-1)
print(&#34;Attention weights:&#34;, attn_weights)
print(&#34;Sum:&#34;, attn_weights.sum(dim=-1))

all_context_vecs = attn_weights @ inputs
print(all_context_vecs)
带可训练权重的自注意力机制（缩放点积注意力 scaled dot-product attention）
关键概念


缩放点积注意力" /><meta name="generator" content="Hugo 0.135.0"><link
    rel="stylesheet"
    href="/css/styles.min.9af39941a3807f10eba8dd56da5fe9f28076ed2722ec76c1aa643b6d55afedbd.css"
    integrity=""
    crossorigin="anonymous"
  />
  
  

  
  <meta property="og:url" content="https://huizhixu.github.io/chs/know_how/20250817%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">
  <meta property="og:site_name" content="徐慧志的个人博客">
  <meta property="og:title" content="2025-08-17 从零构建大模型——注意力机制">
  <meta property="og:description" content="背景 编码器-解码器 编码器将源语言的一串词元序列作为输入，并通过隐藏状态（一个中间神经网络层）编码整个输入序列的压缩表示（可以理解为嵌入）。然后，解码器利用其当前的隐藏状态开始逐个词元进行解码生成。
编码器-解码器RNN的缺陷：在解码阶段，RNN无法直接访问编码器中的早期隐藏状态，它只能依赖当前的隐藏状态。这可能导致上下文丢失，特别是在依赖关系可能跨越较长的距离的句子中。
构建大语言模型的三个阶段 （图来源于书籍）
这张图画得很清晰，第三章的主要学习注意力机制。
学习目标 实现4种注意力机制
简化版的自注意力机制 加入可训练的权重的自注意力机制 因果注意力机制 多头注意力机制 简化版的自注意力机制 关键概念 注意力机制：对于输出，某些输入词元比其他词元更重要。重要性由注意力权重决定。 “自”是什么意思： 上下文向量是什么？ 上下文向量怎么计算？ 实践 import torch inputs = torch.tensor( [[0.43, 0.15, 0.89], # Your (x^1) [0.55, 0.87, 0.66], # journey (x^2) [0.57, 0.85, 0.64], # starts (x^3) [0.22, 0.58, 0.33], # with (x^4) [0.77, 0.25, 0.10], # one (x^5) [0.05, 0.80, 0.55]] # step (x^6) ) print(inputs.shape[0]) query = inputs[1] attn_scores = torch.empty(inputs.shape[0],inputs.shape[0]) attn_scores = inputs @ inputs.T print(attn_scores) attn_weights = torch.softmax(attn_scores, dim=-1) print(&#34;Attention weights:&#34;, attn_weights) print(&#34;Sum:&#34;, attn_weights.sum(dim=-1)) all_context_vecs = attn_weights @ inputs print(all_context_vecs) 带可训练权重的自注意力机制（缩放点积注意力 scaled dot-product attention） 关键概念 缩放点积注意力">
  <meta property="og:locale" content="chs">
  <meta property="og:type" content="article">
    <meta property="article:section" content="know_how">
    <meta property="article:published_time" content="2025-08-17T13:55:21+00:00">
    <meta property="article:modified_time" content="2025-08-17T13:55:21+00:00">
    <meta property="article:tag" content="Tech">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="2025-08-17 从零构建大模型——注意力机制">
  <meta name="twitter:description" content="背景 编码器-解码器 编码器将源语言的一串词元序列作为输入，并通过隐藏状态（一个中间神经网络层）编码整个输入序列的压缩表示（可以理解为嵌入）。然后，解码器利用其当前的隐藏状态开始逐个词元进行解码生成。
编码器-解码器RNN的缺陷：在解码阶段，RNN无法直接访问编码器中的早期隐藏状态，它只能依赖当前的隐藏状态。这可能导致上下文丢失，特别是在依赖关系可能跨越较长的距离的句子中。
构建大语言模型的三个阶段 （图来源于书籍）
这张图画得很清晰，第三章的主要学习注意力机制。
学习目标 实现4种注意力机制
简化版的自注意力机制 加入可训练的权重的自注意力机制 因果注意力机制 多头注意力机制 简化版的自注意力机制 关键概念 注意力机制：对于输出，某些输入词元比其他词元更重要。重要性由注意力权重决定。 “自”是什么意思： 上下文向量是什么？ 上下文向量怎么计算？ 实践 import torch inputs = torch.tensor( [[0.43, 0.15, 0.89], # Your (x^1) [0.55, 0.87, 0.66], # journey (x^2) [0.57, 0.85, 0.64], # starts (x^3) [0.22, 0.58, 0.33], # with (x^4) [0.77, 0.25, 0.10], # one (x^5) [0.05, 0.80, 0.55]] # step (x^6) ) print(inputs.shape[0]) query = inputs[1] attn_scores = torch.empty(inputs.shape[0],inputs.shape[0]) attn_scores = inputs @ inputs.T print(attn_scores) attn_weights = torch.softmax(attn_scores, dim=-1) print(&#34;Attention weights:&#34;, attn_weights) print(&#34;Sum:&#34;, attn_weights.sum(dim=-1)) all_context_vecs = attn_weights @ inputs print(all_context_vecs) 带可训练权重的自注意力机制（缩放点积注意力 scaled dot-product attention） 关键概念 缩放点积注意力">

  
  <meta itemprop="name" content="2025-08-17 从零构建大模型——注意力机制">
  <meta itemprop="description" content="背景 编码器-解码器 编码器将源语言的一串词元序列作为输入，并通过隐藏状态（一个中间神经网络层）编码整个输入序列的压缩表示（可以理解为嵌入）。然后，解码器利用其当前的隐藏状态开始逐个词元进行解码生成。
编码器-解码器RNN的缺陷：在解码阶段，RNN无法直接访问编码器中的早期隐藏状态，它只能依赖当前的隐藏状态。这可能导致上下文丢失，特别是在依赖关系可能跨越较长的距离的句子中。
构建大语言模型的三个阶段 （图来源于书籍）
这张图画得很清晰，第三章的主要学习注意力机制。
学习目标 实现4种注意力机制
简化版的自注意力机制 加入可训练的权重的自注意力机制 因果注意力机制 多头注意力机制 简化版的自注意力机制 关键概念 注意力机制：对于输出，某些输入词元比其他词元更重要。重要性由注意力权重决定。 “自”是什么意思： 上下文向量是什么？ 上下文向量怎么计算？ 实践 import torch inputs = torch.tensor( [[0.43, 0.15, 0.89], # Your (x^1) [0.55, 0.87, 0.66], # journey (x^2) [0.57, 0.85, 0.64], # starts (x^3) [0.22, 0.58, 0.33], # with (x^4) [0.77, 0.25, 0.10], # one (x^5) [0.05, 0.80, 0.55]] # step (x^6) ) print(inputs.shape[0]) query = inputs[1] attn_scores = torch.empty(inputs.shape[0],inputs.shape[0]) attn_scores = inputs @ inputs.T print(attn_scores) attn_weights = torch.softmax(attn_scores, dim=-1) print(&#34;Attention weights:&#34;, attn_weights) print(&#34;Sum:&#34;, attn_weights.sum(dim=-1)) all_context_vecs = attn_weights @ inputs print(all_context_vecs) 带可训练权重的自注意力机制（缩放点积注意力 scaled dot-product attention） 关键概念 缩放点积注意力">
  <meta itemprop="datePublished" content="2025-08-17T13:55:21+00:00">
  <meta itemprop="dateModified" content="2025-08-17T13:55:21+00:00">
  <meta itemprop="wordCount" content="538">
  <meta itemprop="keywords" content="Tech">

  
  <meta name="lang" content="chs" />
  

  
</head>
<body class="dark:bg-gray-800 dark:text-white relative flex flex-col min-h-screen"><header class="container flex justify-between md:justify-between gap-4 flex-wrap p-6 mx-auto relative">
  <a href="https://huizhixu.github.io/chs/" class="capitalize font-extrabold text-2xl">
    
    <img src="../../../blist-logo.png" alt="徐慧志的个人博客" class="h-8 max-w-full" />
    
  </a>
  <button class="mobile-menu-button md:hidden">
    <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <line x1="4" y1="8" x2="20" y2="8" />
      <line x1="4" y1="16" x2="20" y2="16" />
    </svg>
  </button>
  <ul class="mobile-menu absolute z-10 px-6 pb-6 md:p-0 top-full left-0 w-full md:w-auto md:relative hidden md:flex flex-col md:flex-row items-end md:items-center gap-4 lg:gap-6 bg-white dark:bg-gray-800">

    
    <li><a href="../../../chs/know_how/">技术</a></li>
    
    <li><a href="../../../chs/life/">生活见闻</a></li>
    
    <li><a href="../../../chs/page/about/">关于</a></li>
    
    <li><a href="../../../chs/link/">宝藏集结</a></li>
    
    <li><a href="../../../chs/tags/">分类</a></li>
    

    
    
    <li class="relative cursor-pointer">
      <span class="language-switcher flex items-center gap-2">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
          stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="12" cy="12" r="9" />
          <line x1="3.6" y1="9" x2="20.4" y2="9" />
          <line x1="3.6" y1="15" x2="20.4" y2="15" />
          <path d="M11.5 3a17 17 0 0 0 0 18" />
          <path d="M12.5 3a17 17 0 0 1 0 18" />
        </svg>
        <a>语言</a>
        <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14"
          viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round"
          stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <path d="M18 15l-6 -6l-6 6h12" transform="rotate(180 12 12)" />
        </svg>
      </span>
      <div
        class="language-dropdown absolute top-full mt-2 left-0 flex-col gap-2 bg-gray-100 dark:bg-gray-900 dark:text-white z-10 hidden">
        
        
        
        
        <a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href="../../../en/" lang="en">English</a>
        
        
        
        <a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href="../../../de/" lang="de">Deutsch</a>
        
        
      </div>
    </li>
    
    

    
    <li class="grid place-items-center">
      <span class="open-search inline-block cursor-pointer">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="10" cy="10" r="7" />
          <line x1="21" y1="21" x2="15" y2="15" />
        </svg>
      </span>
    </li>
    

    
    <li class="grid place-items-center">
      <span class="toggle-dark-mode inline-block cursor-pointer">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="12" cy="12" r="3" />
          <line x1="12" y1="5" x2="12" y2="5.01" />
          <line x1="17" y1="7" x2="17" y2="7.01" />
          <line x1="19" y1="12" x2="19" y2="12.01" />
          <line x1="17" y1="17" x2="17" y2="17.01" />
          <line x1="12" y1="19" x2="12" y2="19.01" />
          <line x1="7" y1="17" x2="7" y2="17.01" />
          <line x1="5" y1="12" x2="5" y2="12.01" />
          <line x1="7" y1="7" x2="7" y2="7.01" />
        </svg>
      </span>
    </li>
    
  </ul>
</header>
<main class="flex-1">
  
  

  

  <article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4">

    <h1 class="text-2xl font-bold mb-2">2025-08-17 从零构建大模型——注意力机制</h1>
    
    <h5 class="text-sm flex items-center flex-wrap">
      <svg xmlns="http://www.w3.org/2000/svg" class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <rect x="4" y="5" width="16" height="16" rx="2" />
        <line x1="16" y1="3" x2="16" y2="7" />
        <line x1="8" y1="3" x2="8" y2="7" />
        <line x1="4" y1="11" x2="20" y2="11" />
        <rect x="8" y="15" width="2" height="2" />
      </svg>
      发布于 
  
    2025年08月17日
  

      
        &nbsp;&bull;&nbsp;
      
      <svg xmlns="http://www.w3.org/2000/svg" class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <circle cx="12" cy="12" r="9" />
        <polyline points="12 7 12 12 15 15" />
      </svg>
     3&nbsp;分钟
     
      &nbsp;&bull;
      <svg xmlns="http://www.w3.org/2000/svg" class="mx-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <path d="M3 19a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
        <path d="M3 6a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
        <line x1="3" y1="6" x2="3" y2="19" />
        <line x1="12" y1="6" x2="12" y2="19" />
        <line x1="21" y1="6" x2="21" y2="19" />
      </svg>
      538&nbsp;字

    </h5>
    

    <details id="TableOfContents" class="px-4 mt-4 bg-gray-100 dark:bg-gray-700 rounded toc">
    <summary class="flex items-center font-bold py-2 px-4 cursor-pointer justify-between select-none text-black dark:text-white">
      <span>Table of contents</span>
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-down" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
        <polyline points="6 9 12 15 18 9"></polyline>
     </svg>
    </summary>

    <ul class="mt-2 pb-4"><ul>
        

        
        <li>
        <a href="#%e8%83%8c%e6%99%af">背景</a>
        

        
        <ul>
            <li>
        <a href="#%e7%bc%96%e7%a0%81%e5%99%a8-%e8%a7%a3%e7%a0%81%e5%99%a8">编码器-解码器</a>
        

        
        </li></ul>
      </li><li>
        <a href="#%e6%9e%84%e5%bb%ba%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e4%b8%89%e4%b8%aa%e9%98%b6%e6%ae%b5">构建大语言模型的三个阶段</a>
        

        
        </li><li>
        <a href="#%e5%ad%a6%e4%b9%a0%e7%9b%ae%e6%a0%87">学习目标</a>
        

        
        </li><li>
        <a href="#%e7%ae%80%e5%8c%96%e7%89%88%e7%9a%84%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6">简化版的自注意力机制</a>
        

        
        <ul>
            <li>
        <a href="#%e5%85%b3%e9%94%ae%e6%a6%82%e5%bf%b5">关键概念</a>
        

        
        </li><li>
        <a href="#%e5%ae%9e%e8%b7%b5">实践</a>
        

        
        </li></ul>
      </li><li>
        <a href="#%e5%b8%a6%e5%8f%af%e8%ae%ad%e7%bb%83%e6%9d%83%e9%87%8d%e7%9a%84%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e7%bc%a9%e6%94%be%e7%82%b9%e7%a7%af%e6%b3%a8%e6%84%8f%e5%8a%9b-scaled-dot-product-attention">带可训练权重的自注意力机制（缩放点积注意力 scaled dot-product attention）</a>
        

        
        <ul>
            <li>
        <a href="#%e5%85%b3%e9%94%ae%e6%a6%82%e5%bf%b5-1">关键概念</a>
        

        
        </li><li>
        <a href="#%e5%ae%9e%e8%b7%b5-1">实践</a>
        

        
        </li></ul>
      </li><li>
        <a href="#%e5%9b%a0%e6%9e%9c%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6">因果注意力机制</a>
        

        
        <ul>
            <li>
        <a href="#%e5%85%b3%e9%94%ae%e6%a6%82%e5%bf%b5-2">关键概念</a>
        

        
        </li><li>
        <a href="#%e5%ae%9e%e8%b7%b5-2">实践</a>
        

        
        </li></ul>
      </li><li>
        <a href="#%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6">多头注意力机制</a>
        

        
        <ul>
            <li>
        <a href="#%e5%85%b3%e9%94%ae%e6%a6%82%e5%bf%b5-3">关键概念</a>
        

        
        </li><li>
        <a href="#%e5%ae%9e%e8%b7%b5-3">实践</a>
        

        
        </li></ul>
      </li></ul>
          <li>
        <a href="#%e5%a4%9a%e5%a4%b4%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6%e4%bb%a3%e7%a0%81%e4%be%8b%e5%ad%90">多头注意力机制代码例子</a>
        

        
        </li><li>
        <a href="#%e5%85%b6%e4%bb%96">其他</a>
        </li></ul>
  </details>

    <h2 id="背景">背景</h2>
<h3 id="编码器-解码器">编码器-解码器</h3>
<p>编码器将源语言的一串词元序列作为输入，并通过隐藏状态（一个中间神经网络层）编码整个输入序列的压缩表示（可以理解为嵌入）。然后，解码器利用其当前的隐藏状态开始逐个词元进行解码生成。</p>
<p>编码器-解码器RNN的缺陷：在解码阶段，RNN无法直接访问编码器中的早期隐藏状态，它只能依赖当前的隐藏状态。这可能导致上下文丢失，特别是在依赖关系可能跨越较长的距离的句子中。</p>
<h2 id="构建大语言模型的三个阶段">构建大语言模型的三个阶段</h2>
<p><img src="https://raw.githubusercontent.com/HuizhiXu/pictures/master/20250817/bb022129.png"></p>
<p>（图来源于书籍）</p>
<p>这张图画得很清晰，第三章的主要学习注意力机制。</p>
<h2 id="学习目标">学习目标</h2>
<p>实现4种注意力机制</p>
<ol>
<li>简化版的自注意力机制</li>
<li>加入可训练的权重的自注意力机制</li>
<li>因果注意力机制</li>
<li>多头注意力机制</li>
</ol>
<h2 id="简化版的自注意力机制">简化版的自注意力机制</h2>
<h3 id="关键概念">关键概念</h3>
<ol>
<li>注意力机制：对于输出，某些输入词元比其他词元更重要。重要性由注意力权重决定。</li>
<li>“自”是什么意思：</li>
<li>上下文向量是什么？</li>
<li>上下文向量怎么计算？</li>
</ol>
<h3 id="实践">实践</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>import torch
</span></span><span style="display:flex;"><span>inputs = torch.tensor(
</span></span><span style="display:flex;"><span>  [[0.43, 0.15, 0.89], # Your     (x^1)
</span></span><span style="display:flex;"><span>   [0.55, 0.87, 0.66], # journey  (x^2)
</span></span><span style="display:flex;"><span>   [0.57, 0.85, 0.64], # starts   (x^3)
</span></span><span style="display:flex;"><span>   [0.22, 0.58, 0.33], # with     (x^4)
</span></span><span style="display:flex;"><span>   [0.77, 0.25, 0.10], # one      (x^5)
</span></span><span style="display:flex;"><span>   [0.05, 0.80, 0.55]] # step     (x^6)
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(inputs.shape[0])
</span></span><span style="display:flex;"><span>query = inputs[1]
</span></span><span style="display:flex;"><span>attn_scores = torch.empty(inputs.shape[0],inputs.shape[0])
</span></span><span style="display:flex;"><span>attn_scores = inputs @ inputs.T
</span></span><span style="display:flex;"><span>print(attn_scores)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>attn_weights = torch.softmax(attn_scores, dim=-1)
</span></span><span style="display:flex;"><span>print(&#34;Attention weights:&#34;, attn_weights)
</span></span><span style="display:flex;"><span>print(&#34;Sum:&#34;, attn_weights.sum(dim=-1))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>all_context_vecs = attn_weights @ inputs
</span></span><span style="display:flex;"><span>print(all_context_vecs)
</span></span></code></pre></div><h2 id="带可训练权重的自注意力机制缩放点积注意力-scaled-dot-product-attention">带可训练权重的自注意力机制（缩放点积注意力 scaled dot-product attention）</h2>
<h3 id="关键概念-1">关键概念</h3>
<ol>
<li>
<p>缩放点积注意力</p>
</li>
<li>
<p>为什么要用Query 、Key和Value？</p>
</li>
</ol>
<h3 id="实践-1">实践</h3>
<p>v1版：用nn.Parameter</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>import torch.nn as nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>class SelfAttention_v1(nn.Module):
</span></span><span style="display:flex;"><span>    def __init__(self, d_in, d_out):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
</span></span><span style="display:flex;"><span>        self.W_key = nn.Parameter(torch.rand(d_in, d_out))
</span></span><span style="display:flex;"><span>        self.W_value = nn.Parameter(torch.rand(d_in, d_out))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    def forward(self, x):
</span></span><span style="display:flex;"><span>        keys = x @ self.W_key
</span></span><span style="display:flex;"><span>        queries = x @ self.W_query
</span></span><span style="display:flex;"><span>        values = x @ self.W_value
</span></span><span style="display:flex;"><span>        attention_scores = queries @ keys.T
</span></span><span style="display:flex;"><span>        attention_weights = torch.soft(attention_scores/keys.shape[-1]**0.5, dim=-1)
</span></span><span style="display:flex;"><span>        context_vec = attention_weights @ values
</span></span><span style="display:flex;"><span>        return context_vec
</span></span></code></pre></div><p>v2版：用nn.Linear</p>
<p>torch.nn.Parameter仅是一个静态矩阵，若需实现线性变换（如 y = x @ W_query），需手动编写矩阵乘法。nn.Linear是PyTorch 提供的全连接层，自动管理权重和偏置，在初始化策略上与Parameter不一样，因此这两个类对于同一个inputs结果会不一样。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>class SelfAttention_v2(nn.Module):
</span></span><span style="display:flex;"><span>    def __init__(self, d_in, d_out):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>        self.W_query = nn.Linear(d_in, d_out)
</span></span><span style="display:flex;"><span>        self.W_key = nn.Linear(d_in, d_out)
</span></span><span style="display:flex;"><span>        self.W_value = nn.Linear(d_in, d_out)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    def forward(self, x):
</span></span><span style="display:flex;"><span>        keys = self.W_key(x)
</span></span><span style="display:flex;"><span>        queries = self.W_query(x)
</span></span><span style="display:flex;"><span>        values = self.W_value(x)
</span></span><span style="display:flex;"><span>        attention_scores = queries @ keys.T
</span></span><span style="display:flex;"><span>        attention_weights = torch.softmax(attention_scores/keys.shape[-1]**0.5, dim=-1)
</span></span><span style="display:flex;"><span>        context_vec = attention_weights @ values
</span></span><span style="display:flex;"><span>        return context_vec
</span></span></code></pre></div><h2 id="因果注意力机制">因果注意力机制</h2>
<h3 id="关键概念-2">关键概念</h3>
<ol>
<li>因果注意力（也称为掩码注意力）是一种特殊的自注意力形式。它限制模型在处理任何给定词元时，只能基于序列中的先前和当前输入来计算注意力分数，而标准的自注意力机制可以一次性访问整个输入序列</li>
<li>利用dropout掩码额外的注意力权重</li>
</ol>
<h3 id="实践-2">实践</h3>
<ol>
<li>context_length 决定了模型能处理的最长输入序列，用于生成掩码矩阵。</li>
<li>attention_scores = queries @ keys.transpose(1,2) 将批维度保持在第一位。</li>
<li>self.register_buffer 创建一个上三角掩码矩阵（upper triangular mask），并将其注册为模型的缓冲区。</li>
<li>self.mask.bool() 是将 self.mask（一个数值矩阵）转换为 布尔矩阵（Boolean Mask），用于指示哪些位置的注意力分数需要被屏蔽（替换为 -inf），[:num_tokens, :num_tokens]表示截取适配当前序列长度的掩码。</li>
<li>attention_scores.masked_fill_ 带有尾随下划线的操作会就地执行，将True的位置替换为-inf。</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>import torch.nn as nn
</span></span><span style="display:flex;"><span>import torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>class CausalAttention(nn.Module):
</span></span><span style="display:flex;"><span>    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>        self.W_query = nn.Linear(d_in, d_out,bias= qkv_bias)
</span></span><span style="display:flex;"><span>        self.W_key = nn.Linear(d_in, d_out,bias= qkv_bias)
</span></span><span style="display:flex;"><span>        self.W_value = nn.Linear(d_in, d_out,bias= qkv_bias)
</span></span><span style="display:flex;"><span>        self.dropout = nn.Dropout(dropout)
</span></span><span style="display:flex;"><span>        self.register_buffer(
</span></span><span style="display:flex;"><span>            &#39;mask&#39;,
</span></span><span style="display:flex;"><span>            torch.triu(torch.ones(context_length, context_length), diagonal=1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    def forward(self, x):
</span></span><span style="display:flex;"><span>        b,num_tokens,d_in = x.shape
</span></span><span style="display:flex;"><span>        keys = self.W_key(x)
</span></span><span style="display:flex;"><span>        queries = self.W_query(x)
</span></span><span style="display:flex;"><span>        values = self.W_value(x)
</span></span><span style="display:flex;"><span>        attention_scores = queries @ keys.transpose(1,2)
</span></span><span style="display:flex;"><span>        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)
</span></span><span style="display:flex;"><span>        attention_weights = torch.softmax(
</span></span><span style="display:flex;"><span>            attention_scores/keys.shape[-1]**0.5, dim=-1
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        attention_weights = self.dropout(attention_weights)
</span></span><span style="display:flex;"><span>        context_vec = attention_weights @ values
</span></span><span style="display:flex;"><span>        return context_vec
</span></span></code></pre></div><h2 id="多头注意力机制">多头注意力机制</h2>
<h3 id="关键概念-3">关键概念</h3>
<p>多头注意力的主要思想是多次（并行）运行注意力机制，每次使用学到的不同的线性投影。</p>
<p>例如：对于有两个头的多头注意力机制，会使用两组初始权重矩阵，两个查询矩阵，两组注意力权重矩阵，会得到两组上下文向量。</p>
<p>多头注意力机制是指有多组注意力权重。</p>
<h3 id="实践-3">实践</h3>
<p>v1版：用for循环</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>import torch
</span></span><span style="display:flex;"><span>from attention_3 import CausalAttention
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>class MultiHeadAttentionWrapper(nn.Module):
</span></span><span style="display:flex;"><span>    def __init__(self, d_in, d_out, context_length,dropout, num_heads,  qkv_bias=False):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>        self.heads = nn.ModuleList(
</span></span><span style="display:flex;"><span>            [CausalAttention(d_in, d_out,context_length,dropout, qkv_bias) for _ in range(num_heads)]
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    def forward(self,x):
</span></span><span style="display:flex;"><span>        return torch.cat([head(x) for head in self.heads], dim=-1)
</span></span></code></pre></div><p>v2版：用批量矩阵乘法</p>
<ol>
<li>对 keys、queries 和 values 进行维度重塑（reshape）：将 (b, num_tokens, d_model) 重塑为 (b, num_tokens, num_heads, head_dim)。</li>
<li>self.out_proj 的作用。多头注意力中，每个头独立计算特征（如 head1 关注语法，head2 关注语义），直接拼接（torch.cat）只是简单堆叠，而 out_proj 通过权重矩阵动态混合这些特征。</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plain" data-lang="plain"><span style="display:flex;"><span>import torch.nn as nn
</span></span><span style="display:flex;"><span>import torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>class MultiHeadAttention(nn.Module):
</span></span><span style="display:flex;"><span>    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>        assert (d_out % num_heads == 0), &#34;d_out must be divisible by num_heads&#34;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self.d_out=d_out
</span></span><span style="display:flex;"><span>        self.num_heads = num_heads
</span></span><span style="display:flex;"><span>        self.head_dim = d_out // num_heads
</span></span><span style="display:flex;"><span>        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
</span></span><span style="display:flex;"><span>        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
</span></span><span style="display:flex;"><span>        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
</span></span><span style="display:flex;"><span>        self.out_proj = nn.Linear(d_out, d_out)
</span></span><span style="display:flex;"><span>        self.dropout = nn.Dropout(dropout)
</span></span><span style="display:flex;"><span>        self.register_buffer(
</span></span><span style="display:flex;"><span>            &#39;mask&#39;,
</span></span><span style="display:flex;"><span>            torch.triu(torch.ones(context_length, context_length), diagonal=1)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    def forward(self,x):
</span></span><span style="display:flex;"><span>        b,num_tokens,d_in = x.shape
</span></span><span style="display:flex;"><span>        keys = self.W_key(x)
</span></span><span style="display:flex;"><span>        queries = self.W_query(x)
</span></span><span style="display:flex;"><span>        values = self.W_value(x)
</span></span><span style="display:flex;"><span>        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
</span></span><span style="display:flex;"><span>        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)
</span></span><span style="display:flex;"><span>        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        keys = keys.transpose(1,2)
</span></span><span style="display:flex;"><span>        queries = queries.transpose(1,2)
</span></span><span style="display:flex;"><span>        values = values.transpose(1,2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        attention_scores = queries @ keys.transpose(2,3)
</span></span><span style="display:flex;"><span>        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)
</span></span><span style="display:flex;"><span>        attention_weights = torch.softmax(
</span></span><span style="display:flex;"><span>            attention_scores/keys.shape[-1]**0.5, dim=-1
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        attention_weights = self.dropout(attention_weights)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        context_vec = attention_weights @ values
</span></span><span style="display:flex;"><span>        context_vec = context_vec.transpose(1,2)
</span></span><span style="display:flex;"><span>        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)
</span></span><span style="display:flex;"><span>        context_vec = self.out_proj(context_vec)
</span></span><span style="display:flex;"><span>        return context_vec
</span></span></code></pre></div><h1 id="多头注意力机制代码例子">多头注意力机制代码例子</h1>
<p>设batch_size=2，num_tokens=6，d_in=512，d_out=512，num_heads=8，head_dim=64。</p>
<ol>
<li>输入形状</li>
<li>生成 Q/K/V</li>
<li>拆分成多头（view + transpose）</li>
<li>计算注意力分数</li>
<li>应用因果掩码</li>
<li>计算注意力权重</li>
<li>计算上下文向量</li>
<li>合并多头输出</li>
</ol>
<h1 id="其他">其他</h1>
<p>softmax函数可以保证注意力权重总是正值，这使得输出可以被解释为概率或相对重要性，其中权重越高表示重要程度越高。</p>

  </article>
<div class="px-2 mb-2">
  
  <script src="https://utteranc.es/client.js"
    repo="HuizhiXu/huizhixu.github.io"
    issue-term="pathname"
    theme="github-light"
    crossorigin="anonymous"
    async>
  </script>
  
</div>
<div class="bg-blue-100 dark:bg-gray-900">
  <div class="container px-4 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-4 items-center">
    <div>
      <div class="text-2xl font-bold mb-2">Sein heißt werden, leben heißt lernen.</div>
      <p class="opacity-60">Der einfache Weg is immer verkehrt.</p>
    </div>

    <ul class="flex justify-center gap-x-3 flex-wrap gap-y-2">
      

      
      <li>
        <a
          href="https://twitter.com/"
          target="_blank"
          rel="noopener"
          aria-label="Twitter"
          class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            fill="none"
            stroke-linecap="round"
            stroke-linejoin="round"
          >
            <path stroke="none" d="M0 0h24v24H0z" fill="none" />
            <path
              d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z"
            />
          </svg>
        </a>
      </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      
      <li>
        <a
          href="https://github.com/"
          target="_blank"
          rel="noopener"
          aria-label="GitHub"
          class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            fill="none"
            stroke-linecap="round"
            stroke-linejoin="round"
          >
            <path stroke="none" d="M0 0h24v24H0z" fill="none" />
            <path
              d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5"
            />
          </svg>
        </a>
      </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      
    </ul>
  </div>
</div>

    </main><footer class="container p-6 mx-auto flex justify-between items-center">
  <span class="text-sm font-light">
    
    Copyright © 2012 - Huizhi Xu · All rights reserved
    
  </span>
  <span onclick="window.scrollTo({top: 0, behavior: 'smooth'})" class="p-1 cursor-pointer">
    <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5"
      stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M18 15l-6 -6l-6 6h12" />
    </svg>
  </span>
</footer>

<div class="search-ui absolute top-0 left-0 w-full h-full bg-white dark:bg-gray-800 hidden">
  <div class="container max-w-3xl mx-auto p-12">
    <div class="relative">
      <div class="my-4 text-center text-2xl font-bold">Search</div>

      <span class="p-2 absolute right-0 top-0 cursor-pointer close-search">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <line x1="18" y1="6" x2="6" y2="18" />
          <line x1="6" y1="6" x2="18" y2="18" />
        </svg>
      </span>
    </div>

    <input type="search" class="py-2 px-3 w-full dark:text-black border dark:border-transparent"
      placeholder="Enter search query" />

    <div class="search-results text-lg font-medium my-4 hidden">Results</div>
    <ul class="search-list my-2">

    </ul>

    <div class="no-results text-center my-8 hidden">
      <div class="text-xl font-semibold mb-2">No results found</div>
      <p class="font-light text-sm">Try adjusting your search query</p>
    </div>
  </div>
</div>





<script src="https://huizhixu.github.io/js/scripts.min.js"></script>




<script>
  const languageMenuButton = document.querySelector('.language-switcher');
  const languageDropdown = document.querySelector('.language-dropdown');
  languageMenuButton.addEventListener('click', (evt) => {
    evt.preventDefault()
    if (languageDropdown.classList.contains('hidden')) {
      languageDropdown.classList.remove('hidden')
      languageDropdown.classList.add('flex')
    } else {
      languageDropdown.classList.add('hidden');
      languageDropdown.classList.remove('flex');
    }
  })
</script>



<script>
  
  const darkmode = document.querySelector('.toggle-dark-mode');
  function toggleDarkMode() {
    if (document.documentElement.classList.contains('dark')) {
      document.documentElement.classList.remove('dark')
      localStorage.setItem('darkmode', 'light')
    } else {
      document.documentElement.classList.add('dark')
      localStorage.setItem('darkmode', 'dark')
    }
  }
  if (darkmode) {
    darkmode.addEventListener('click', toggleDarkMode);
  }

  const darkStorage = localStorage.getItem('darkmode');
  const isBrowserDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;

  if (!darkStorage && isBrowserDark) {
    document.documentElement.classList.add('dark');
  }

  if (darkStorage && darkStorage === 'dark') {
    toggleDarkMode();
  }
</script>


<script>
  const mobileMenuButton = document.querySelector('.mobile-menu-button')
  const mobileMenu = document.querySelector('.mobile-menu')
  function toggleMenu() {
    mobileMenu.classList.toggle('hidden');
    mobileMenu.classList.toggle('flex');
  }
  if(mobileMenu && mobileMenuButton){
    mobileMenuButton.addEventListener('click', toggleMenu)
  }
</script>
</body>
</html>
