<!DOCTYPE html>
<html lang="chs" itemscope itemtype="http://schema.org/WebPage"><head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="../../../favicon.svg">

  <title>
  2025-07-02 GraphRAG实践 - 徐慧志的个人博客
  </title>
  <meta name="description" content="GraphRAG刚出来的时候，使用的是OpenAI的对话模型和向量模型，由于在过程中会使用非常多的调用，所以成本较为昂贵。如果想用便宜的国产大模型或者在本地部署，免费使用，那就涉及到本地模型的使用。
使用其他模型有很多种方式，可以用ollama, slang, vllm，text-generations等方式部署。
GraphRAG改版非常多次，在1.0.0版本不支持使用本地模型，在最新的2.0.0以上的版本开始支持ollam部署的模型。
如果要用OpenAI之外的对话模型和向量模型，建议详细阅读：https://github.com/microsoft/graphrag/issues/657。
我本次用的是ollama部署的qwen2:7b和本地部署的embedding模型。
提前准备
1. 安装环境
conda create -n graphrag3.10 python=3.10
oonda activate graphrag3.10
2. 准备好ollama模型
ollama run qwen2:7b
使用步骤
1. 下载graphrag
git clone git@github.com:microsoft/graphrag.git
cd graphrag
pip install -e.
2. 初始化
准备数据输入文件夹
mkdir -p ./graphrag_sophia/input  # 注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改
初始化
python -m graphrag init --root ./graphrag_sophia
可以看见在graphrag_sophia下生成settings.yaml和prompts文件夹。
这一步还需要把要把*.txt文件放入input文件夹中
3. 生成索引
3.1 准备数据
先准备数据，在graph_rag文件夹新建input，将.txt文件放进去。如果手头没有.txt文件，可以用官方提供的文件。
mkdir -p ./input （注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改）
curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt -o ./data/test/book.txt
3.2 设置workspace variables
我在查阅资料的过程中，发现很多人的settings.yaml是这样的：
llm:
model: qwen2:7b
api_base: http://127.0.0.1:11434/v1
model_supports_json: false
max_tokens: 32768
在配置这一步卡了很久，反复报错。原因是新版必须包含三个字段：default_chat_model、type和api_key。官网没给示例，我试了很多写法都不对，估计是因为版本差异。现在我的解决方式是：在.env和settings.yaml里分别设置api_base和api_key。
我这里的设置：在.env文件中GRAPHRAG_API_KEY=&lt;API_KEY&gt;
settings.yaml文件：
### This config file contains required core defaults that must be set, along with a handful of common optional settings.
### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/

### LLM settings ###
## There are a number of settings to tune the threading and token limits for LLM calls - check the docs.

models:
  default_chat_model:
    type: openai_chat # or azure_openai_chat
    api_base: http://127.0.0.1:11434/v1
    auth_type: api_key # or azure_managed_identity
    api_key: ollama # set this in the generated .env file
    model: qwen2:7b
    encoding_model: cl100k_base # automatically set by tiktoken if left undefined
    model_supports_json: true # recommended if this is available for your model.
    concurrent_requests: 25 # max number of simultaneous LLM requests allowed
    async_mode: threaded # or asyncio
    retry_strategy: native
    max_retries: 10
    tokens_per_minute: auto              # set to null to disable rate limiting
    requests_per_minute: auto            # set to null to disable rate limiting
  default_embedding_model:
    type: openai_embedding # or azure_openai_embedding
    api_base: http://127.0.0.1:8005/api/v1 
    auth_type: api_key # or azure_managed_identity
    api_key: ollama
    model: bge
    encoding_model: cl100k_base # automatically set by tiktoken if left undefined
    model_supports_json: true # recommended if this is available for your model.
    concurrent_requests: 25 # max number of simultaneous LLM requests allowed
    async_mode: threaded # or asyncio
    retry_strategy: native
    max_retries: 10
    tokens_per_minute: auto              # set to null to disable rate limiting
    requests_per_minute: auto            # set to null to disable rate limiting

### Input settings ###

input:
  type: file # or blob
  file_type: text # [csv, text, json]
  base_dir: &#34;input&#34;

chunks:
  size: 200 #1200
  overlap: 50 # 100 
  group_by_columns: [id]

### Output/storage settings ###
## If blob storage is specified in the following four sections,
## connection_string and container_name must be provided

output:
  type: file # [file, blob, cosmosdb]
  base_dir: &#34;output&#34;
    
cache:
  type: file # [file, blob, cosmosdb]
  base_dir: &#34;cache&#34;

reporting:
  type: file # [file, blob, cosmosdb]
  base_dir: &#34;logs&#34;

vector_store:
  default_vector_store:
    type: lancedb
    db_uri: output\lancedb
    container_name: default
    overwrite: True

### Workflow settings ###

embed_text:
  model_id: default_embedding_model
  vector_store_id: default_vector_store

extract_graph:
  model_id: default_chat_model
  prompt: &#34;prompts/extract_graph.txt&#34;
  entity_types: [organization,person,geo,event]
  max_gleanings: 1

summarize_descriptions:
  model_id: default_chat_model
  prompt: &#34;prompts/summarize_descriptions.txt&#34;
  max_length: 500

extract_graph_nlp:
  text_analyzer:
    extractor_type: regex_english # [regex_english, syntactic_parser, cfg]

cluster_graph:
  max_cluster_size: 10

extract_claims:
  enabled: false
  model_id: default_chat_model
  prompt: &#34;prompts/extract_claims.txt&#34;
  description: &#34;Any claims or facts that could be relevant to information discovery.&#34;
  max_gleanings: 1

community_reports:
  model_id: default_chat_model
  graph_prompt: &#34;prompts/community_report_graph.txt&#34;
  text_prompt: &#34;prompts/community_report_text.txt&#34;
  max_length: 2000
  max_input_length: 8000

embed_graph:
  enabled: false # if true, will generate node2vec embeddings for nodes

umap:
  enabled: false # if true, will generate UMAP embeddings for nodes (embed_graph must also be enabled)

snapshots:
  graphml: false
  embeddings: false

### Query settings ###
## The prompt locations are required here, but each search method has a number of optional knobs that can be tuned.
## See the config docs: https://microsoft.github.io/graphrag/config/yaml/#query

local_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model
  prompt: &#34;prompts/local_search_system_prompt.txt&#34;

global_search:
  chat_model_id: default_chat_model
  map_prompt: &#34;prompts/global_search_map_system_prompt.txt&#34;
  reduce_prompt: &#34;prompts/global_search_reduce_system_prompt.txt&#34;
  knowledge_prompt: &#34;prompts/global_search_knowledge_system_prompt.txt&#34;

drift_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model
  prompt: &#34;prompts/drift_search_system_prompt.txt&#34;
  reduce_prompt: &#34;prompts/drift_search_reduce_prompt.txt&#34;

basic_search:
  chat_model_id: default_chat_model
  embedding_model_id: default_embedding_model
  prompt: &#34;prompts/basic_search_system_prompt.txt&#34;
3.2 生成索引
这个过程非常非常慢，且token的消耗巨大。（在chunk_length为100的情况下，final_documents每100个14分钟，extract_graph一个小时进度35%）" /><meta name="generator" content="Hugo 0.135.0"><link
    rel="stylesheet"
    href="/css/styles.min.9af39941a3807f10eba8dd56da5fe9f28076ed2722ec76c1aa643b6d55afedbd.css"
    integrity=""
    crossorigin="anonymous"
  />
  
  

  
  <meta property="og:url" content="https://huizhixu.github.io/chs/know_how/20250702-graphrag%E5%AE%9E%E8%B7%B5/">
  <meta property="og:site_name" content="徐慧志的个人博客">
  <meta property="og:title" content="2025-07-02 GraphRAG实践">
  <meta property="og:description" content="GraphRAG刚出来的时候，使用的是OpenAI的对话模型和向量模型，由于在过程中会使用非常多的调用，所以成本较为昂贵。如果想用便宜的国产大模型或者在本地部署，免费使用，那就涉及到本地模型的使用。 使用其他模型有很多种方式，可以用ollama, slang, vllm，text-generations等方式部署。 GraphRAG改版非常多次，在1.0.0版本不支持使用本地模型，在最新的2.0.0以上的版本开始支持ollam部署的模型。 如果要用OpenAI之外的对话模型和向量模型，建议详细阅读：https://github.com/microsoft/graphrag/issues/657。
我本次用的是ollama部署的qwen2:7b和本地部署的embedding模型。
提前准备 1. 安装环境 conda create -n graphrag3.10 python=3.10 oonda activate graphrag3.10 2. 准备好ollama模型 ollama run qwen2:7b 使用步骤 1. 下载graphrag git clone git@github.com:microsoft/graphrag.git cd graphrag pip install -e. 2. 初始化 准备数据输入文件夹
mkdir -p ./graphrag_sophia/input # 注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改 初始化
python -m graphrag init --root ./graphrag_sophia 可以看见在graphrag_sophia下生成settings.yaml和prompts文件夹。 这一步还需要把要把*.txt文件放入input文件夹中
3. 生成索引 3.1 准备数据 先准备数据，在graph_rag文件夹新建input，将.txt文件放进去。如果手头没有.txt文件，可以用官方提供的文件。 mkdir -p ./input （注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改）
curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt -o ./data/test/book.txt 3.2 设置workspace variables
我在查阅资料的过程中，发现很多人的settings.yaml是这样的：
llm: model: qwen2:7b api_base: http://127.0.0.1:11434/v1 model_supports_json: false max_tokens: 32768 在配置这一步卡了很久，反复报错。原因是新版必须包含三个字段：default_chat_model、type和api_key。官网没给示例，我试了很多写法都不对，估计是因为版本差异。现在我的解决方式是：在.env和settings.yaml里分别设置api_base和api_key。
我这里的设置：在.env文件中GRAPHRAG_API_KEY=&lt;API_KEY&gt; settings.yaml文件：
### This config file contains required core defaults that must be set, along with a handful of common optional settings. ### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/ ### LLM settings ### ## There are a number of settings to tune the threading and token limits for LLM calls - check the docs. models: default_chat_model: type: openai_chat # or azure_openai_chat api_base: http://127.0.0.1:11434/v1 auth_type: api_key # or azure_managed_identity api_key: ollama # set this in the generated .env file model: qwen2:7b encoding_model: cl100k_base # automatically set by tiktoken if left undefined model_supports_json: true # recommended if this is available for your model. concurrent_requests: 25 # max number of simultaneous LLM requests allowed async_mode: threaded # or asyncio retry_strategy: native max_retries: 10 tokens_per_minute: auto # set to null to disable rate limiting requests_per_minute: auto # set to null to disable rate limiting default_embedding_model: type: openai_embedding # or azure_openai_embedding api_base: http://127.0.0.1:8005/api/v1 auth_type: api_key # or azure_managed_identity api_key: ollama model: bge encoding_model: cl100k_base # automatically set by tiktoken if left undefined model_supports_json: true # recommended if this is available for your model. concurrent_requests: 25 # max number of simultaneous LLM requests allowed async_mode: threaded # or asyncio retry_strategy: native max_retries: 10 tokens_per_minute: auto # set to null to disable rate limiting requests_per_minute: auto # set to null to disable rate limiting ### Input settings ### input: type: file # or blob file_type: text # [csv, text, json] base_dir: &#34;input&#34; chunks: size: 200 #1200 overlap: 50 # 100 group_by_columns: [id] ### Output/storage settings ### ## If blob storage is specified in the following four sections, ## connection_string and container_name must be provided output: type: file # [file, blob, cosmosdb] base_dir: &#34;output&#34; cache: type: file # [file, blob, cosmosdb] base_dir: &#34;cache&#34; reporting: type: file # [file, blob, cosmosdb] base_dir: &#34;logs&#34; vector_store: default_vector_store: type: lancedb db_uri: output\lancedb container_name: default overwrite: True ### Workflow settings ### embed_text: model_id: default_embedding_model vector_store_id: default_vector_store extract_graph: model_id: default_chat_model prompt: &#34;prompts/extract_graph.txt&#34; entity_types: [organization,person,geo,event] max_gleanings: 1 summarize_descriptions: model_id: default_chat_model prompt: &#34;prompts/summarize_descriptions.txt&#34; max_length: 500 extract_graph_nlp: text_analyzer: extractor_type: regex_english # [regex_english, syntactic_parser, cfg] cluster_graph: max_cluster_size: 10 extract_claims: enabled: false model_id: default_chat_model prompt: &#34;prompts/extract_claims.txt&#34; description: &#34;Any claims or facts that could be relevant to information discovery.&#34; max_gleanings: 1 community_reports: model_id: default_chat_model graph_prompt: &#34;prompts/community_report_graph.txt&#34; text_prompt: &#34;prompts/community_report_text.txt&#34; max_length: 2000 max_input_length: 8000 embed_graph: enabled: false # if true, will generate node2vec embeddings for nodes umap: enabled: false # if true, will generate UMAP embeddings for nodes (embed_graph must also be enabled) snapshots: graphml: false embeddings: false ### Query settings ### ## The prompt locations are required here, but each search method has a number of optional knobs that can be tuned. ## See the config docs: https://microsoft.github.io/graphrag/config/yaml/#query local_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/local_search_system_prompt.txt&#34; global_search: chat_model_id: default_chat_model map_prompt: &#34;prompts/global_search_map_system_prompt.txt&#34; reduce_prompt: &#34;prompts/global_search_reduce_system_prompt.txt&#34; knowledge_prompt: &#34;prompts/global_search_knowledge_system_prompt.txt&#34; drift_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/drift_search_system_prompt.txt&#34; reduce_prompt: &#34;prompts/drift_search_reduce_prompt.txt&#34; basic_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/basic_search_system_prompt.txt&#34; 3.2 生成索引 这个过程非常非常慢，且token的消耗巨大。（在chunk_length为100的情况下，final_documents每100个14分钟，extract_graph一个小时进度35%）">
  <meta property="og:locale" content="chs">
  <meta property="og:type" content="article">
    <meta property="article:section" content="know_how">
    <meta property="article:published_time" content="2025-07-02T13:54:51+00:00">
    <meta property="article:modified_time" content="2025-07-02T13:54:51+00:00">
    <meta property="article:tag" content="Tech">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="2025-07-02 GraphRAG实践">
  <meta name="twitter:description" content="GraphRAG刚出来的时候，使用的是OpenAI的对话模型和向量模型，由于在过程中会使用非常多的调用，所以成本较为昂贵。如果想用便宜的国产大模型或者在本地部署，免费使用，那就涉及到本地模型的使用。 使用其他模型有很多种方式，可以用ollama, slang, vllm，text-generations等方式部署。 GraphRAG改版非常多次，在1.0.0版本不支持使用本地模型，在最新的2.0.0以上的版本开始支持ollam部署的模型。 如果要用OpenAI之外的对话模型和向量模型，建议详细阅读：https://github.com/microsoft/graphrag/issues/657。
我本次用的是ollama部署的qwen2:7b和本地部署的embedding模型。
提前准备 1. 安装环境 conda create -n graphrag3.10 python=3.10 oonda activate graphrag3.10 2. 准备好ollama模型 ollama run qwen2:7b 使用步骤 1. 下载graphrag git clone git@github.com:microsoft/graphrag.git cd graphrag pip install -e. 2. 初始化 准备数据输入文件夹
mkdir -p ./graphrag_sophia/input # 注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改 初始化
python -m graphrag init --root ./graphrag_sophia 可以看见在graphrag_sophia下生成settings.yaml和prompts文件夹。 这一步还需要把要把*.txt文件放入input文件夹中
3. 生成索引 3.1 准备数据 先准备数据，在graph_rag文件夹新建input，将.txt文件放进去。如果手头没有.txt文件，可以用官方提供的文件。 mkdir -p ./input （注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改）
curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt -o ./data/test/book.txt 3.2 设置workspace variables
我在查阅资料的过程中，发现很多人的settings.yaml是这样的：
llm: model: qwen2:7b api_base: http://127.0.0.1:11434/v1 model_supports_json: false max_tokens: 32768 在配置这一步卡了很久，反复报错。原因是新版必须包含三个字段：default_chat_model、type和api_key。官网没给示例，我试了很多写法都不对，估计是因为版本差异。现在我的解决方式是：在.env和settings.yaml里分别设置api_base和api_key。
我这里的设置：在.env文件中GRAPHRAG_API_KEY=&lt;API_KEY&gt; settings.yaml文件：
### This config file contains required core defaults that must be set, along with a handful of common optional settings. ### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/ ### LLM settings ### ## There are a number of settings to tune the threading and token limits for LLM calls - check the docs. models: default_chat_model: type: openai_chat # or azure_openai_chat api_base: http://127.0.0.1:11434/v1 auth_type: api_key # or azure_managed_identity api_key: ollama # set this in the generated .env file model: qwen2:7b encoding_model: cl100k_base # automatically set by tiktoken if left undefined model_supports_json: true # recommended if this is available for your model. concurrent_requests: 25 # max number of simultaneous LLM requests allowed async_mode: threaded # or asyncio retry_strategy: native max_retries: 10 tokens_per_minute: auto # set to null to disable rate limiting requests_per_minute: auto # set to null to disable rate limiting default_embedding_model: type: openai_embedding # or azure_openai_embedding api_base: http://127.0.0.1:8005/api/v1 auth_type: api_key # or azure_managed_identity api_key: ollama model: bge encoding_model: cl100k_base # automatically set by tiktoken if left undefined model_supports_json: true # recommended if this is available for your model. concurrent_requests: 25 # max number of simultaneous LLM requests allowed async_mode: threaded # or asyncio retry_strategy: native max_retries: 10 tokens_per_minute: auto # set to null to disable rate limiting requests_per_minute: auto # set to null to disable rate limiting ### Input settings ### input: type: file # or blob file_type: text # [csv, text, json] base_dir: &#34;input&#34; chunks: size: 200 #1200 overlap: 50 # 100 group_by_columns: [id] ### Output/storage settings ### ## If blob storage is specified in the following four sections, ## connection_string and container_name must be provided output: type: file # [file, blob, cosmosdb] base_dir: &#34;output&#34; cache: type: file # [file, blob, cosmosdb] base_dir: &#34;cache&#34; reporting: type: file # [file, blob, cosmosdb] base_dir: &#34;logs&#34; vector_store: default_vector_store: type: lancedb db_uri: output\lancedb container_name: default overwrite: True ### Workflow settings ### embed_text: model_id: default_embedding_model vector_store_id: default_vector_store extract_graph: model_id: default_chat_model prompt: &#34;prompts/extract_graph.txt&#34; entity_types: [organization,person,geo,event] max_gleanings: 1 summarize_descriptions: model_id: default_chat_model prompt: &#34;prompts/summarize_descriptions.txt&#34; max_length: 500 extract_graph_nlp: text_analyzer: extractor_type: regex_english # [regex_english, syntactic_parser, cfg] cluster_graph: max_cluster_size: 10 extract_claims: enabled: false model_id: default_chat_model prompt: &#34;prompts/extract_claims.txt&#34; description: &#34;Any claims or facts that could be relevant to information discovery.&#34; max_gleanings: 1 community_reports: model_id: default_chat_model graph_prompt: &#34;prompts/community_report_graph.txt&#34; text_prompt: &#34;prompts/community_report_text.txt&#34; max_length: 2000 max_input_length: 8000 embed_graph: enabled: false # if true, will generate node2vec embeddings for nodes umap: enabled: false # if true, will generate UMAP embeddings for nodes (embed_graph must also be enabled) snapshots: graphml: false embeddings: false ### Query settings ### ## The prompt locations are required here, but each search method has a number of optional knobs that can be tuned. ## See the config docs: https://microsoft.github.io/graphrag/config/yaml/#query local_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/local_search_system_prompt.txt&#34; global_search: chat_model_id: default_chat_model map_prompt: &#34;prompts/global_search_map_system_prompt.txt&#34; reduce_prompt: &#34;prompts/global_search_reduce_system_prompt.txt&#34; knowledge_prompt: &#34;prompts/global_search_knowledge_system_prompt.txt&#34; drift_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/drift_search_system_prompt.txt&#34; reduce_prompt: &#34;prompts/drift_search_reduce_prompt.txt&#34; basic_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/basic_search_system_prompt.txt&#34; 3.2 生成索引 这个过程非常非常慢，且token的消耗巨大。（在chunk_length为100的情况下，final_documents每100个14分钟，extract_graph一个小时进度35%）">

  
  <meta itemprop="name" content="2025-07-02 GraphRAG实践">
  <meta itemprop="description" content="GraphRAG刚出来的时候，使用的是OpenAI的对话模型和向量模型，由于在过程中会使用非常多的调用，所以成本较为昂贵。如果想用便宜的国产大模型或者在本地部署，免费使用，那就涉及到本地模型的使用。 使用其他模型有很多种方式，可以用ollama, slang, vllm，text-generations等方式部署。 GraphRAG改版非常多次，在1.0.0版本不支持使用本地模型，在最新的2.0.0以上的版本开始支持ollam部署的模型。 如果要用OpenAI之外的对话模型和向量模型，建议详细阅读：https://github.com/microsoft/graphrag/issues/657。
我本次用的是ollama部署的qwen2:7b和本地部署的embedding模型。
提前准备 1. 安装环境 conda create -n graphrag3.10 python=3.10 oonda activate graphrag3.10 2. 准备好ollama模型 ollama run qwen2:7b 使用步骤 1. 下载graphrag git clone git@github.com:microsoft/graphrag.git cd graphrag pip install -e. 2. 初始化 准备数据输入文件夹
mkdir -p ./graphrag_sophia/input # 注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改 初始化
python -m graphrag init --root ./graphrag_sophia 可以看见在graphrag_sophia下生成settings.yaml和prompts文件夹。 这一步还需要把要把*.txt文件放入input文件夹中
3. 生成索引 3.1 准备数据 先准备数据，在graph_rag文件夹新建input，将.txt文件放进去。如果手头没有.txt文件，可以用官方提供的文件。 mkdir -p ./input （注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改）
curl https://www.gutenberg.org/cache/epub/24022/pg24022.txt -o ./data/test/book.txt 3.2 设置workspace variables
我在查阅资料的过程中，发现很多人的settings.yaml是这样的：
llm: model: qwen2:7b api_base: http://127.0.0.1:11434/v1 model_supports_json: false max_tokens: 32768 在配置这一步卡了很久，反复报错。原因是新版必须包含三个字段：default_chat_model、type和api_key。官网没给示例，我试了很多写法都不对，估计是因为版本差异。现在我的解决方式是：在.env和settings.yaml里分别设置api_base和api_key。
我这里的设置：在.env文件中GRAPHRAG_API_KEY=&lt;API_KEY&gt; settings.yaml文件：
### This config file contains required core defaults that must be set, along with a handful of common optional settings. ### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/ ### LLM settings ### ## There are a number of settings to tune the threading and token limits for LLM calls - check the docs. models: default_chat_model: type: openai_chat # or azure_openai_chat api_base: http://127.0.0.1:11434/v1 auth_type: api_key # or azure_managed_identity api_key: ollama # set this in the generated .env file model: qwen2:7b encoding_model: cl100k_base # automatically set by tiktoken if left undefined model_supports_json: true # recommended if this is available for your model. concurrent_requests: 25 # max number of simultaneous LLM requests allowed async_mode: threaded # or asyncio retry_strategy: native max_retries: 10 tokens_per_minute: auto # set to null to disable rate limiting requests_per_minute: auto # set to null to disable rate limiting default_embedding_model: type: openai_embedding # or azure_openai_embedding api_base: http://127.0.0.1:8005/api/v1 auth_type: api_key # or azure_managed_identity api_key: ollama model: bge encoding_model: cl100k_base # automatically set by tiktoken if left undefined model_supports_json: true # recommended if this is available for your model. concurrent_requests: 25 # max number of simultaneous LLM requests allowed async_mode: threaded # or asyncio retry_strategy: native max_retries: 10 tokens_per_minute: auto # set to null to disable rate limiting requests_per_minute: auto # set to null to disable rate limiting ### Input settings ### input: type: file # or blob file_type: text # [csv, text, json] base_dir: &#34;input&#34; chunks: size: 200 #1200 overlap: 50 # 100 group_by_columns: [id] ### Output/storage settings ### ## If blob storage is specified in the following four sections, ## connection_string and container_name must be provided output: type: file # [file, blob, cosmosdb] base_dir: &#34;output&#34; cache: type: file # [file, blob, cosmosdb] base_dir: &#34;cache&#34; reporting: type: file # [file, blob, cosmosdb] base_dir: &#34;logs&#34; vector_store: default_vector_store: type: lancedb db_uri: output\lancedb container_name: default overwrite: True ### Workflow settings ### embed_text: model_id: default_embedding_model vector_store_id: default_vector_store extract_graph: model_id: default_chat_model prompt: &#34;prompts/extract_graph.txt&#34; entity_types: [organization,person,geo,event] max_gleanings: 1 summarize_descriptions: model_id: default_chat_model prompt: &#34;prompts/summarize_descriptions.txt&#34; max_length: 500 extract_graph_nlp: text_analyzer: extractor_type: regex_english # [regex_english, syntactic_parser, cfg] cluster_graph: max_cluster_size: 10 extract_claims: enabled: false model_id: default_chat_model prompt: &#34;prompts/extract_claims.txt&#34; description: &#34;Any claims or facts that could be relevant to information discovery.&#34; max_gleanings: 1 community_reports: model_id: default_chat_model graph_prompt: &#34;prompts/community_report_graph.txt&#34; text_prompt: &#34;prompts/community_report_text.txt&#34; max_length: 2000 max_input_length: 8000 embed_graph: enabled: false # if true, will generate node2vec embeddings for nodes umap: enabled: false # if true, will generate UMAP embeddings for nodes (embed_graph must also be enabled) snapshots: graphml: false embeddings: false ### Query settings ### ## The prompt locations are required here, but each search method has a number of optional knobs that can be tuned. ## See the config docs: https://microsoft.github.io/graphrag/config/yaml/#query local_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/local_search_system_prompt.txt&#34; global_search: chat_model_id: default_chat_model map_prompt: &#34;prompts/global_search_map_system_prompt.txt&#34; reduce_prompt: &#34;prompts/global_search_reduce_system_prompt.txt&#34; knowledge_prompt: &#34;prompts/global_search_knowledge_system_prompt.txt&#34; drift_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/drift_search_system_prompt.txt&#34; reduce_prompt: &#34;prompts/drift_search_reduce_prompt.txt&#34; basic_search: chat_model_id: default_chat_model embedding_model_id: default_embedding_model prompt: &#34;prompts/basic_search_system_prompt.txt&#34; 3.2 生成索引 这个过程非常非常慢，且token的消耗巨大。（在chunk_length为100的情况下，final_documents每100个14分钟，extract_graph一个小时进度35%）">
  <meta itemprop="datePublished" content="2025-07-02T13:54:51+00:00">
  <meta itemprop="dateModified" content="2025-07-02T13:54:51+00:00">
  <meta itemprop="wordCount" content="991">
  <meta itemprop="keywords" content="Tech">

  
  <meta name="lang" content="chs" />
  

  
</head>
<body class="dark:bg-gray-800 dark:text-white relative flex flex-col min-h-screen"><header class="container flex justify-between md:justify-between gap-4 flex-wrap p-6 mx-auto relative">
  <a href="https://huizhixu.github.io/chs/" class="capitalize font-extrabold text-2xl">
    
    <img src="../../../blist-logo.png" alt="徐慧志的个人博客" class="h-8 max-w-full" />
    
  </a>
  <button class="mobile-menu-button md:hidden">
    <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <line x1="4" y1="8" x2="20" y2="8" />
      <line x1="4" y1="16" x2="20" y2="16" />
    </svg>
  </button>
  <ul class="mobile-menu absolute z-10 px-6 pb-6 md:p-0 top-full left-0 w-full md:w-auto md:relative hidden md:flex flex-col md:flex-row items-end md:items-center gap-4 lg:gap-6 bg-white dark:bg-gray-800">

    
    <li><a href="../../../chs/know_how/">技术</a></li>
    
    <li><a href="../../../chs/life/">生活见闻</a></li>
    
    <li><a href="../../../chs/page/about/">关于</a></li>
    
    <li><a href="../../../chs/link/">宝藏集结</a></li>
    
    <li><a href="../../../chs/tags/">分类</a></li>
    

    
    
    <li class="relative cursor-pointer">
      <span class="language-switcher flex items-center gap-2">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24"
          stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="12" cy="12" r="9" />
          <line x1="3.6" y1="9" x2="20.4" y2="9" />
          <line x1="3.6" y1="15" x2="20.4" y2="15" />
          <path d="M11.5 3a17 17 0 0 0 0 18" />
          <path d="M12.5 3a17 17 0 0 1 0 18" />
        </svg>
        <a>语言</a>
        <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14"
          viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round"
          stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <path d="M18 15l-6 -6l-6 6h12" transform="rotate(180 12 12)" />
        </svg>
      </span>
      <div
        class="language-dropdown absolute top-full mt-2 left-0 flex-col gap-2 bg-gray-100 dark:bg-gray-900 dark:text-white z-10 hidden">
        
        
        
        
        <a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href="../../../en/" lang="en">English</a>
        
        
        
        <a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href="../../../de/" lang="de">Deutsch</a>
        
        
      </div>
    </li>
    
    

    
    <li class="grid place-items-center">
      <span class="open-search inline-block cursor-pointer">
        <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="10" cy="10" r="7" />
          <line x1="21" y1="21" x2="15" y2="15" />
        </svg>
      </span>
    </li>
    

    
    <li class="grid place-items-center">
      <span class="toggle-dark-mode inline-block cursor-pointer">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <circle cx="12" cy="12" r="3" />
          <line x1="12" y1="5" x2="12" y2="5.01" />
          <line x1="17" y1="7" x2="17" y2="7.01" />
          <line x1="19" y1="12" x2="19" y2="12.01" />
          <line x1="17" y1="17" x2="17" y2="17.01" />
          <line x1="12" y1="19" x2="12" y2="19.01" />
          <line x1="7" y1="17" x2="7" y2="17.01" />
          <line x1="5" y1="12" x2="5" y2="12.01" />
          <line x1="7" y1="7" x2="7" y2="7.01" />
        </svg>
      </span>
    </li>
    
  </ul>
</header>
<main class="flex-1">
  
  

  

  <article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4">

    <h1 class="text-2xl font-bold mb-2">2025-07-02 GraphRAG实践</h1>
    
    <h5 class="text-sm flex items-center flex-wrap">
      <svg xmlns="http://www.w3.org/2000/svg" class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <rect x="4" y="5" width="16" height="16" rx="2" />
        <line x1="16" y1="3" x2="16" y2="7" />
        <line x1="8" y1="3" x2="8" y2="7" />
        <line x1="4" y1="11" x2="20" y2="11" />
        <rect x="8" y="15" width="2" height="2" />
      </svg>
      发布于 
  
    2025年07月02日
  

      
        &nbsp;&bull;&nbsp;
      
      <svg xmlns="http://www.w3.org/2000/svg" class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <circle cx="12" cy="12" r="9" />
        <polyline points="12 7 12 12 15 15" />
      </svg>
     5&nbsp;分钟
     
      &nbsp;&bull;
      <svg xmlns="http://www.w3.org/2000/svg" class="mx-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <path d="M3 19a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
        <path d="M3 6a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
        <line x1="3" y1="6" x2="3" y2="19" />
        <line x1="12" y1="6" x2="12" y2="19" />
        <line x1="21" y1="6" x2="21" y2="19" />
      </svg>
      991&nbsp;字

    </h5>
    

    <details id="TableOfContents" class="px-4 mt-4 bg-gray-100 dark:bg-gray-700 rounded toc">
    <summary class="flex items-center font-bold py-2 px-4 cursor-pointer justify-between select-none text-black dark:text-white">
      <span>Table of contents</span>
      <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-down" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
        <polyline points="6 9 12 15 18 9"></polyline>
     </svg>
    </summary>

    <ul class="mt-2 pb-4">
        

        
        <li>
        <a href="#%e6%8f%90%e5%89%8d%e5%87%86%e5%a4%87">提前准备</a>
        

        
        <ul>
            <ul>
            <li>
        <a href="#1-%e5%ae%89%e8%a3%85%e7%8e%af%e5%a2%83">1. 安装环境</a>
        

        
        </li><li>
        <a href="#2-%e5%87%86%e5%a4%87%e5%a5%bdollama%e6%a8%a1%e5%9e%8b">2. 准备好ollama模型</a>
        

        
        </li></ul>
          </ul>
      </li><li>
        <a href="#%e4%bd%bf%e7%94%a8%e6%ad%a5%e9%aa%a4">使用步骤</a>
        

        
        <ul>
            <ul>
            <li>
        <a href="#1-%e4%b8%8b%e8%bd%bdgraphrag">1. 下载graphrag</a>
        

        
        </li><li>
        <a href="#2-%e5%88%9d%e5%a7%8b%e5%8c%96">2. 初始化</a>
        

        
        </li><li>
        <a href="#3-%e7%94%9f%e6%88%90%e7%b4%a2%e5%bc%95">3. 生成索引</a>
        </li></ul></ul>
    </li></ul>
  </details>

    <p>GraphRAG刚出来的时候，使用的是OpenAI的对话模型和向量模型，由于在过程中会使用非常多的调用，所以成本较为昂贵。如果想用便宜的国产大模型或者在本地部署，免费使用，那就涉及到本地模型的使用。
使用其他模型有很多种方式，可以用ollama, slang, vllm，text-generations等方式部署。
GraphRAG改版非常多次，在1.0.0版本不支持使用本地模型，在最新的2.0.0以上的版本开始支持ollam部署的模型。
如果要用OpenAI之外的对话模型和向量模型，建议详细阅读：https://github.com/microsoft/graphrag/issues/657。</p>
<p>我本次用的是ollama部署的qwen2:7b和本地部署的embedding模型。</p>
<h1 id="提前准备">提前准备</h1>
<h3 id="1-安装环境">1. 安装环境</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>conda create <span style="color:#ff79c6">-</span>n graphrag3<span style="color:#bd93f9">.10</span> python<span style="color:#ff79c6">=</span><span style="color:#bd93f9">3.10</span>
</span></span><span style="display:flex;"><span>oonda activate graphrag3<span style="color:#bd93f9">.10</span>
</span></span></code></pre></div><h3 id="2-准备好ollama模型">2. 准备好ollama模型</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ollama run qwen2:<span style="color:#bd93f9">7</span>b
</span></span></code></pre></div><h1 id="使用步骤">使用步骤</h1>
<h3 id="1-下载graphrag">1. 下载graphrag</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>git clone git@github.com:microsoft<span style="color:#ff79c6">/</span>graphrag<span style="color:#ff79c6">.</span>git
</span></span><span style="display:flex;"><span>cd graphrag
</span></span><span style="display:flex;"><span>pip install <span style="color:#ff79c6">-</span>e<span style="color:#ff79c6">.</span>
</span></span></code></pre></div><h3 id="2-初始化">2. 初始化</h3>
<p>准备数据输入文件夹</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mkdir <span style="color:#ff79c6">-</span>p <span style="color:#ff79c6">./</span>graphrag_sophia<span style="color:#ff79c6">/</span><span style="color:#8be9fd;font-style:italic">input</span>  <span style="color:#6272a4"># 注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改</span>
</span></span></code></pre></div><p>初始化</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>python <span style="color:#ff79c6">-</span>m graphrag init <span style="color:#ff79c6">--</span>root <span style="color:#ff79c6">./</span>graphrag_sophia
</span></span></code></pre></div><p>可以看见在graphrag_sophia下生成settings.yaml和prompts文件夹。
这一步还需要把要把*.txt文件放入input文件夹中</p>
<h3 id="3-生成索引">3. 生成索引</h3>
<p>3.1 准备数据
先准备数据，在graph_rag文件夹新建input，将.txt文件放进去。如果手头没有.txt文件，可以用官方提供的文件。
mkdir -p ./input （注意一定要建立一个input子文件夹，在创建索引的时候会去识别这个名字。如果是其他名字，可以在settings.yaml更改）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>curl https:<span style="color:#ff79c6">//</span>www<span style="color:#ff79c6">.</span>gutenberg<span style="color:#ff79c6">.</span>org<span style="color:#ff79c6">/</span>cache<span style="color:#ff79c6">/</span>epub<span style="color:#ff79c6">/</span><span style="color:#bd93f9">24022</span><span style="color:#ff79c6">/</span>pg24022<span style="color:#ff79c6">.</span>txt <span style="color:#ff79c6">-</span>o <span style="color:#ff79c6">./</span>data<span style="color:#ff79c6">/</span>test<span style="color:#ff79c6">/</span>book<span style="color:#ff79c6">.</span>txt
</span></span></code></pre></div><p>3.2 设置workspace variables</p>
<p>我在查阅资料的过程中，发现很多人的settings.yaml是这样的：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>llm:
</span></span><span style="display:flex;"><span>model: qwen2:<span style="color:#bd93f9">7</span>b
</span></span><span style="display:flex;"><span>api_base: http:<span style="color:#ff79c6">//</span><span style="color:#bd93f9">127.0.0.1</span>:<span style="color:#bd93f9">11434</span><span style="color:#ff79c6">/</span>v1
</span></span><span style="display:flex;"><span>model_supports_json: false
</span></span><span style="display:flex;"><span>max_tokens: <span style="color:#bd93f9">32768</span>
</span></span></code></pre></div><p>在配置这一步卡了很久，反复报错。原因是新版必须包含三个字段：default_chat_model、type和api_key。官网没给示例，我试了很多写法都不对，估计是因为版本差异。现在我的解决方式是：在.env和settings.yaml里分别设置api_base和api_key。</p>
<p>我这里的设置：在.env文件中GRAPHRAG_API_KEY=&lt;API_KEY&gt;
settings.yaml文件：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#6272a4">### This config file contains required core defaults that must be set, along with a handful of common optional settings.</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### For a full list of available settings, see https://microsoft.github.io/graphrag/config/yaml/</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### LLM settings ###</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">## There are a number of settings to tune the threading and token limits for LLM calls - check the docs.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>models:
</span></span><span style="display:flex;"><span>  default_chat_model:
</span></span><span style="display:flex;"><span>    <span style="color:#8be9fd;font-style:italic">type</span>: openai_chat <span style="color:#6272a4"># or azure_openai_chat</span>
</span></span><span style="display:flex;"><span>    api_base: http:<span style="color:#ff79c6">//</span><span style="color:#bd93f9">127.0.0.1</span>:<span style="color:#bd93f9">11434</span><span style="color:#ff79c6">/</span>v1
</span></span><span style="display:flex;"><span>    auth_type: api_key <span style="color:#6272a4"># or azure_managed_identity</span>
</span></span><span style="display:flex;"><span>    api_key: ollama <span style="color:#6272a4"># set this in the generated .env file</span>
</span></span><span style="display:flex;"><span>    model: qwen2:<span style="color:#bd93f9">7</span>b
</span></span><span style="display:flex;"><span>    encoding_model: cl100k_base <span style="color:#6272a4"># automatically set by tiktoken if left undefined</span>
</span></span><span style="display:flex;"><span>    model_supports_json: true <span style="color:#6272a4"># recommended if this is available for your model.</span>
</span></span><span style="display:flex;"><span>    concurrent_requests: <span style="color:#bd93f9">25</span> <span style="color:#6272a4"># max number of simultaneous LLM requests allowed</span>
</span></span><span style="display:flex;"><span>    async_mode: threaded <span style="color:#6272a4"># or asyncio</span>
</span></span><span style="display:flex;"><span>    retry_strategy: native
</span></span><span style="display:flex;"><span>    max_retries: <span style="color:#bd93f9">10</span>
</span></span><span style="display:flex;"><span>    tokens_per_minute: auto              <span style="color:#6272a4"># set to null to disable rate limiting</span>
</span></span><span style="display:flex;"><span>    requests_per_minute: auto            <span style="color:#6272a4"># set to null to disable rate limiting</span>
</span></span><span style="display:flex;"><span>  default_embedding_model:
</span></span><span style="display:flex;"><span>    <span style="color:#8be9fd;font-style:italic">type</span>: openai_embedding <span style="color:#6272a4"># or azure_openai_embedding</span>
</span></span><span style="display:flex;"><span>    api_base: http:<span style="color:#ff79c6">//</span><span style="color:#bd93f9">127.0.0.1</span>:<span style="color:#bd93f9">8005</span><span style="color:#ff79c6">/</span>api<span style="color:#ff79c6">/</span>v1 
</span></span><span style="display:flex;"><span>    auth_type: api_key <span style="color:#6272a4"># or azure_managed_identity</span>
</span></span><span style="display:flex;"><span>    api_key: ollama
</span></span><span style="display:flex;"><span>    model: bge
</span></span><span style="display:flex;"><span>    encoding_model: cl100k_base <span style="color:#6272a4"># automatically set by tiktoken if left undefined</span>
</span></span><span style="display:flex;"><span>    model_supports_json: true <span style="color:#6272a4"># recommended if this is available for your model.</span>
</span></span><span style="display:flex;"><span>    concurrent_requests: <span style="color:#bd93f9">25</span> <span style="color:#6272a4"># max number of simultaneous LLM requests allowed</span>
</span></span><span style="display:flex;"><span>    async_mode: threaded <span style="color:#6272a4"># or asyncio</span>
</span></span><span style="display:flex;"><span>    retry_strategy: native
</span></span><span style="display:flex;"><span>    max_retries: <span style="color:#bd93f9">10</span>
</span></span><span style="display:flex;"><span>    tokens_per_minute: auto              <span style="color:#6272a4"># set to null to disable rate limiting</span>
</span></span><span style="display:flex;"><span>    requests_per_minute: auto            <span style="color:#6272a4"># set to null to disable rate limiting</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### Input settings ###</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">input</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#8be9fd;font-style:italic">type</span>: file <span style="color:#6272a4"># or blob</span>
</span></span><span style="display:flex;"><span>  file_type: text <span style="color:#6272a4"># [csv, text, json]</span>
</span></span><span style="display:flex;"><span>  base_dir: <span style="color:#f1fa8c">&#34;input&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>chunks:
</span></span><span style="display:flex;"><span>  size: <span style="color:#bd93f9">200</span> <span style="color:#6272a4">#1200</span>
</span></span><span style="display:flex;"><span>  overlap: <span style="color:#bd93f9">50</span> <span style="color:#6272a4"># 100 </span>
</span></span><span style="display:flex;"><span>  group_by_columns: [<span style="color:#8be9fd;font-style:italic">id</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### Output/storage settings ###</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">## If blob storage is specified in the following four sections,</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">## connection_string and container_name must be provided</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output:
</span></span><span style="display:flex;"><span>  <span style="color:#8be9fd;font-style:italic">type</span>: file <span style="color:#6272a4"># [file, blob, cosmosdb]</span>
</span></span><span style="display:flex;"><span>  base_dir: <span style="color:#f1fa8c">&#34;output&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>cache:
</span></span><span style="display:flex;"><span>  <span style="color:#8be9fd;font-style:italic">type</span>: file <span style="color:#6272a4"># [file, blob, cosmosdb]</span>
</span></span><span style="display:flex;"><span>  base_dir: <span style="color:#f1fa8c">&#34;cache&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>reporting:
</span></span><span style="display:flex;"><span>  <span style="color:#8be9fd;font-style:italic">type</span>: file <span style="color:#6272a4"># [file, blob, cosmosdb]</span>
</span></span><span style="display:flex;"><span>  base_dir: <span style="color:#f1fa8c">&#34;logs&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vector_store:
</span></span><span style="display:flex;"><span>  default_vector_store:
</span></span><span style="display:flex;"><span>    <span style="color:#8be9fd;font-style:italic">type</span>: lancedb
</span></span><span style="display:flex;"><span>    db_uri: output\lancedb
</span></span><span style="display:flex;"><span>    container_name: default
</span></span><span style="display:flex;"><span>    overwrite: <span style="color:#ff79c6">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### Workflow settings ###</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embed_text:
</span></span><span style="display:flex;"><span>  model_id: default_embedding_model
</span></span><span style="display:flex;"><span>  vector_store_id: default_vector_store
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>extract_graph:
</span></span><span style="display:flex;"><span>  model_id: default_chat_model
</span></span><span style="display:flex;"><span>  prompt: <span style="color:#f1fa8c">&#34;prompts/extract_graph.txt&#34;</span>
</span></span><span style="display:flex;"><span>  entity_types: [organization,person,geo,event]
</span></span><span style="display:flex;"><span>  max_gleanings: <span style="color:#bd93f9">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>summarize_descriptions:
</span></span><span style="display:flex;"><span>  model_id: default_chat_model
</span></span><span style="display:flex;"><span>  prompt: <span style="color:#f1fa8c">&#34;prompts/summarize_descriptions.txt&#34;</span>
</span></span><span style="display:flex;"><span>  max_length: <span style="color:#bd93f9">500</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>extract_graph_nlp:
</span></span><span style="display:flex;"><span>  text_analyzer:
</span></span><span style="display:flex;"><span>    extractor_type: regex_english <span style="color:#6272a4"># [regex_english, syntactic_parser, cfg]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cluster_graph:
</span></span><span style="display:flex;"><span>  max_cluster_size: <span style="color:#bd93f9">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>extract_claims:
</span></span><span style="display:flex;"><span>  enabled: false
</span></span><span style="display:flex;"><span>  model_id: default_chat_model
</span></span><span style="display:flex;"><span>  prompt: <span style="color:#f1fa8c">&#34;prompts/extract_claims.txt&#34;</span>
</span></span><span style="display:flex;"><span>  description: <span style="color:#f1fa8c">&#34;Any claims or facts that could be relevant to information discovery.&#34;</span>
</span></span><span style="display:flex;"><span>  max_gleanings: <span style="color:#bd93f9">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>community_reports:
</span></span><span style="display:flex;"><span>  model_id: default_chat_model
</span></span><span style="display:flex;"><span>  graph_prompt: <span style="color:#f1fa8c">&#34;prompts/community_report_graph.txt&#34;</span>
</span></span><span style="display:flex;"><span>  text_prompt: <span style="color:#f1fa8c">&#34;prompts/community_report_text.txt&#34;</span>
</span></span><span style="display:flex;"><span>  max_length: <span style="color:#bd93f9">2000</span>
</span></span><span style="display:flex;"><span>  max_input_length: <span style="color:#bd93f9">8000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embed_graph:
</span></span><span style="display:flex;"><span>  enabled: false <span style="color:#6272a4"># if true, will generate node2vec embeddings for nodes</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>umap:
</span></span><span style="display:flex;"><span>  enabled: false <span style="color:#6272a4"># if true, will generate UMAP embeddings for nodes (embed_graph must also be enabled)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>snapshots:
</span></span><span style="display:flex;"><span>  graphml: false
</span></span><span style="display:flex;"><span>  embeddings: false
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### Query settings ###</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">## The prompt locations are required here, but each search method has a number of optional knobs that can be tuned.</span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">## See the config docs: https://microsoft.github.io/graphrag/config/yaml/#query</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>local_search:
</span></span><span style="display:flex;"><span>  chat_model_id: default_chat_model
</span></span><span style="display:flex;"><span>  embedding_model_id: default_embedding_model
</span></span><span style="display:flex;"><span>  prompt: <span style="color:#f1fa8c">&#34;prompts/local_search_system_prompt.txt&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>global_search:
</span></span><span style="display:flex;"><span>  chat_model_id: default_chat_model
</span></span><span style="display:flex;"><span>  map_prompt: <span style="color:#f1fa8c">&#34;prompts/global_search_map_system_prompt.txt&#34;</span>
</span></span><span style="display:flex;"><span>  reduce_prompt: <span style="color:#f1fa8c">&#34;prompts/global_search_reduce_system_prompt.txt&#34;</span>
</span></span><span style="display:flex;"><span>  knowledge_prompt: <span style="color:#f1fa8c">&#34;prompts/global_search_knowledge_system_prompt.txt&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>drift_search:
</span></span><span style="display:flex;"><span>  chat_model_id: default_chat_model
</span></span><span style="display:flex;"><span>  embedding_model_id: default_embedding_model
</span></span><span style="display:flex;"><span>  prompt: <span style="color:#f1fa8c">&#34;prompts/drift_search_system_prompt.txt&#34;</span>
</span></span><span style="display:flex;"><span>  reduce_prompt: <span style="color:#f1fa8c">&#34;prompts/drift_search_reduce_prompt.txt&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>basic_search:
</span></span><span style="display:flex;"><span>  chat_model_id: default_chat_model
</span></span><span style="display:flex;"><span>  embedding_model_id: default_embedding_model
</span></span><span style="display:flex;"><span>  prompt: <span style="color:#f1fa8c">&#34;prompts/basic_search_system_prompt.txt&#34;</span>
</span></span></code></pre></div><p>3.2 生成索引
这个过程非常非常慢，且token的消耗巨大。（在chunk_length为100的情况下，final_documents每100个14分钟，extract_graph一个小时进度35%）</p>
<p>运行这个的前提是数据放在graph_rag/input下，在graph_rag下运行
python -m graphrag index &ndash;root .\graphrag_sophia<br>
看到这个说明索引建立是ok的。</p>
<p><img src="https://raw.githubusercontent.com/HuizhiXu/pictures/master/20250702/cb1f67e3.png"></p>
<p>如果这一步有问题，说明api_base有可能出错，对于ollama来说，model的后缀需要去掉，或者从api改为v1。对于本地部署的embedding模型来说，embedding的后缀也要去掉。另外，不能用ollama来运行bge-m3，原因在这：https://segmentfault.com/a/1190000046497918。</p>
<p>如果有错误，可以到ollama的 view logs和 graphrag\graphrag_sophia\logs\indexing-engine.log查看日志。</p>
<p>3.3 查询
查询有三种，lcoal search, global search以及drift search</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>graphrag query <span style="color:#ff79c6">--</span>root <span style="color:#ff79c6">.</span>\graphrag_sophia <span style="color:#ff79c6">--</span>method <span style="color:#ff79c6">global</span> <span style="color:#ff79c6">--</span>query <span style="color:#f1fa8c">&#34;Please introduce the Forbidden City&#34;</span>
</span></span></code></pre></div><p>结果</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Warning: All <span style="color:#8be9fd;font-style:italic">map</span> responses have score <span style="color:#bd93f9">0</span> (i<span style="color:#ff79c6">.</span>e<span style="color:#ff79c6">.</span>, no relevant information found <span style="color:#ff79c6">from</span> the dataset), returning a canned <span style="color:#f1fa8c">&#39;I do not know&#39;</span> answer<span style="color:#ff79c6">.</span> You can <span style="color:#ff79c6">try</span> enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant general knowledge, at the risk of increasing hallucinations<span style="color:#ff79c6">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>SUCCESS: Global Search Response:
</span></span><span style="display:flex;"><span>I am sorry but I am unable to answer this question given the provided data<span style="color:#ff79c6">.</span>
</span></span></code></pre></div><p>当method为local的时候</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>INFO: Vector Store Args: {
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;default_vector_store&#34;</span>: {
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;type&#34;</span>: <span style="color:#f1fa8c">&#34;lancedb&#34;</span>,
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;db_uri&#34;</span>: <span style="color:#f1fa8c">&#34;C:</span><span style="color:#f1fa8c">\\</span><span style="color:#f1fa8c">Users</span><span style="color:#f1fa8c">\\</span><span style="color:#f1fa8c">Sophia</span><span style="color:#f1fa8c">\\</span><span style="color:#f1fa8c">graphrag</span><span style="color:#f1fa8c">\\</span><span style="color:#f1fa8c">graphrag_sophia</span><span style="color:#f1fa8c">\\</span><span style="color:#f1fa8c">output</span><span style="color:#f1fa8c">\\</span><span style="color:#f1fa8c">lancedb&#34;</span>,
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;url&#34;</span>: null,
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;audience&#34;</span>: null,
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;container_name&#34;</span>: <span style="color:#f1fa8c">&#34;==== REDACTED ====&#34;</span>,
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;database_name&#34;</span>: null,
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">&#34;overwrite&#34;</span>: true
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>SUCCESS: Local Search Response:
</span></span><span style="display:flex;"><span>The Forbidden City, officially known <span style="color:#ff79c6">as</span> the Palace Museum, <span style="color:#ff79c6">is</span> a historic palace <span style="color:#8be9fd;font-style:italic">complex</span> located <span style="color:#ff79c6">in</span> the heart of Beijing, China<span style="color:#ff79c6">.</span> It was constructed during the Ming dynasty (<span style="color:#bd93f9">1406</span><span style="color:#ff79c6">-</span><span style="color:#bd93f9">1420</span>) <span style="color:#ff79c6">and</span> served <span style="color:#ff79c6">as</span> the imperial palace <span style="color:#ff79c6">for</span> <span style="color:#bd93f9">24</span> emperors over the course of the Ming <span style="color:#ff79c6">and</span> Qing dynasties (<span style="color:#bd93f9">1368</span><span style="color:#ff79c6">-</span><span style="color:#bd93f9">1912</span>)<span style="color:#ff79c6">.</span> The Forbidden City covers an area of approximately <span style="color:#bd93f9">720</span>,<span style="color:#bd93f9">000</span> square meters <span style="color:#ff79c6">and</span> <span style="color:#ff79c6">is</span> surrounded by a large moat<span style="color:#ff79c6">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>The <span style="color:#8be9fd;font-style:italic">complex</span> <span style="color:#ff79c6">is</span> divided into two main sections: the Outer Court <span style="color:#ff79c6">and</span> the Inner Court<span style="color:#ff79c6">.</span> The Outer Court was where the emperor conducted official business <span style="color:#ff79c6">and</span> received foreign dignitants, <span style="color:#ff79c6">while</span> the Inner Court was reserved <span style="color:#ff79c6">for</span> the imperial family<span style="color:#f1fa8c">&#39;s private life, including the emperor, empresses, concubines, and their children.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>The Forbidden City <span style="color:#ff79c6">is</span> renowned <span style="color:#ff79c6">for</span> its grand architecture, which showcases traditional Chinese building styles <span style="color:#ff79c6">with</span> intricate details such <span style="color:#ff79c6">as</span> roof tiles decorated <span style="color:#ff79c6">with</span> dragons, phoenixes, <span style="color:#ff79c6">and</span> other mythical creatures<span style="color:#ff79c6">.</span> The palace buildings are arranged <span style="color:#ff79c6">in</span> a symmetrical layout, reflecting the imperial ideology of harmony <span style="color:#ff79c6">and</span> balance<span style="color:#ff79c6">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Today, the Forbidden City serves <span style="color:#ff79c6">as</span> a museum that houses over <span style="color:#bd93f9">1</span> million artifacts <span style="color:#ff79c6">and</span> artworks <span style="color:#ff79c6">from</span> various dynasties<span style="color:#ff79c6">.</span> It <span style="color:#ff79c6">is</span> one of the most visited tourist attractions <span style="color:#ff79c6">in</span> China <span style="color:#ff79c6">and</span> <span style="color:#ff79c6">is</span> recognized <span style="color:#ff79c6">as</span> a UNESCO World Heritage Site <span style="color:#ff79c6">for</span> its cultural significance <span style="color:#ff79c6">and</span> historical value<span style="color:#ff79c6">.</span> The palace <span style="color:#8be9fd;font-style:italic">complex</span> <span style="color:#ff79c6">not</span> only preserves the legacy of ancient Chinese architecture but also provides insights into the imperial lifestyle, court rituals, <span style="color:#ff79c6">and</span> the political structure of the Ming <span style="color:#ff79c6">and</span> Qing dynasties<span style="color:#ff79c6">.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Visitors can explore various halls, palaces, gardens, <span style="color:#ff79c6">and</span> courtyards within the Forbidden City, each <span style="color:#ff79c6">with</span> its own unique history <span style="color:#ff79c6">and</span> stories<span style="color:#ff79c6">.</span> Some notable structures include the Hall of Supreme Harmony (Zhonghe Dian), the Hall of Central Harmony (Zhongheng Dian), the Hall of Earthly Tranquility (Taixing Dian), <span style="color:#ff79c6">and</span> the Palace of Heavenly Purity (Tianqing Gong)<span style="color:#ff79c6">.</span> The palace also hosts special exhibitions that showcase Chinese art, calligraphy, ceramics, <span style="color:#ff79c6">and</span> other cultural treasures<span style="color:#ff79c6">.</span>
</span></span></code></pre></div><p>上面只是原生的GraphRAG，可以看出可以优化的地方非常多。首先，Global search结果为空，其次GraphRAG 支持语言为英文，需要改为中文，另外速度太慢也是问题。</p>

  </article>
<div class="px-2 mb-2">
  
  <script src="https://utteranc.es/client.js"
    repo="HuizhiXu/huizhixu.github.io"
    issue-term="pathname"
    theme="github-light"
    crossorigin="anonymous"
    async>
  </script>
  
</div>
<div class="bg-blue-100 dark:bg-gray-900">
  <div class="container px-4 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-4 items-center">
    <div>
      <div class="text-2xl font-bold mb-2">Sein heißt werden, leben heißt lernen.</div>
      <p class="opacity-60">Der einfache Weg is immer verkehrt.</p>
    </div>

    <ul class="flex justify-center gap-x-3 flex-wrap gap-y-2">
      

      
      <li>
        <a
          href="https://twitter.com/"
          target="_blank"
          rel="noopener"
          aria-label="Twitter"
          class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            fill="none"
            stroke-linecap="round"
            stroke-linejoin="round"
          >
            <path stroke="none" d="M0 0h24v24H0z" fill="none" />
            <path
              d="M22 4.01c-1 .49 -1.98 .689 -3 .99c-1.121 -1.265 -2.783 -1.335 -4.38 -.737s-2.643 2.06 -2.62 3.737v1c-3.245 .083 -6.135 -1.395 -8 -4c0 0 -4.182 7.433 4 11c-1.872 1.247 -3.739 2.088 -6 2c3.308 1.803 6.913 2.423 10.034 1.517c3.58 -1.04 6.522 -3.723 7.651 -7.742a13.84 13.84 0 0 0 .497 -3.753c-.002 -.249 1.51 -2.772 1.818 -4.013z"
            />
          </svg>
        </a>
      </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      
      <li>
        <a
          href="https://github.com/"
          target="_blank"
          rel="noopener"
          aria-label="GitHub"
          class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
            fill="none"
            stroke-linecap="round"
            stroke-linejoin="round"
          >
            <path stroke="none" d="M0 0h24v24H0z" fill="none" />
            <path
              d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5"
            />
          </svg>
        </a>
      </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      
    </ul>
  </div>
</div>

    </main><footer class="container p-6 mx-auto flex justify-between items-center">
  <span class="text-sm font-light">
    
    Copyright © 2012 - Huizhi Xu · All rights reserved
    
  </span>
  <span onclick="window.scrollTo({top: 0, behavior: 'smooth'})" class="p-1 cursor-pointer">
    <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5"
      stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M18 15l-6 -6l-6 6h12" />
    </svg>
  </span>
</footer>

<div class="search-ui absolute top-0 left-0 w-full h-full bg-white dark:bg-gray-800 hidden">
  <div class="container max-w-3xl mx-auto p-12">
    <div class="relative">
      <div class="my-4 text-center text-2xl font-bold">Search</div>

      <span class="p-2 absolute right-0 top-0 cursor-pointer close-search">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5"
          stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
          <path stroke="none" d="M0 0h24v24H0z" fill="none" />
          <line x1="18" y1="6" x2="6" y2="18" />
          <line x1="6" y1="6" x2="18" y2="18" />
        </svg>
      </span>
    </div>

    <input type="search" class="py-2 px-3 w-full dark:text-black border dark:border-transparent"
      placeholder="Enter search query" />

    <div class="search-results text-lg font-medium my-4 hidden">Results</div>
    <ul class="search-list my-2">

    </ul>

    <div class="no-results text-center my-8 hidden">
      <div class="text-xl font-semibold mb-2">No results found</div>
      <p class="font-light text-sm">Try adjusting your search query</p>
    </div>
  </div>
</div>





<script src="https://huizhixu.github.io/js/scripts.min.js"></script>




<script>
  const languageMenuButton = document.querySelector('.language-switcher');
  const languageDropdown = document.querySelector('.language-dropdown');
  languageMenuButton.addEventListener('click', (evt) => {
    evt.preventDefault()
    if (languageDropdown.classList.contains('hidden')) {
      languageDropdown.classList.remove('hidden')
      languageDropdown.classList.add('flex')
    } else {
      languageDropdown.classList.add('hidden');
      languageDropdown.classList.remove('flex');
    }
  })
</script>



<script>
  
  const darkmode = document.querySelector('.toggle-dark-mode');
  function toggleDarkMode() {
    if (document.documentElement.classList.contains('dark')) {
      document.documentElement.classList.remove('dark')
      localStorage.setItem('darkmode', 'light')
    } else {
      document.documentElement.classList.add('dark')
      localStorage.setItem('darkmode', 'dark')
    }
  }
  if (darkmode) {
    darkmode.addEventListener('click', toggleDarkMode);
  }

  const darkStorage = localStorage.getItem('darkmode');
  const isBrowserDark = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;

  if (!darkStorage && isBrowserDark) {
    document.documentElement.classList.add('dark');
  }

  if (darkStorage && darkStorage === 'dark') {
    toggleDarkMode();
  }
</script>


<script>
  const mobileMenuButton = document.querySelector('.mobile-menu-button')
  const mobileMenu = document.querySelector('.mobile-menu')
  function toggleMenu() {
    mobileMenu.classList.toggle('hidden');
    mobileMenu.classList.toggle('flex');
  }
  if(mobileMenu && mobileMenuButton){
    mobileMenuButton.addEventListener('click', toggleMenu)
  }
</script>
</body>
</html>
